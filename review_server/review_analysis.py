"""
 review_analysis.py
- Airbnb ìˆ™ì†Œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•´
- BERTopicìœ¼ë¡œ í† í”½ ëª¨ë¸ë§ ë° ì£¼ì œë³„ ë¬¸ì¥ ë¶„ë¥˜
"""

import base64, re, requests, json
import pandas as pd
from bs4 import BeautifulSoup
from collections import defaultdict

# ëª¨ë¸ ë° ë¶„ì„ ë„êµ¬
import torch
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
import hdbscan
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import kss
from urllib.parse import quote, urlencode


# ==========================================
#  í•œê¸€ ë¦¬ë·° í˜¹ì€ ë²ˆì—­ë³¸ ì¶”ì¶œ í•¨ìˆ˜
# ==========================================
def comments(r):
    """
    í•œê¸€ ë¦¬ë·° ë˜ëŠ” ë²ˆì—­ë³¸ ë¦¬ë·°ë¥¼ ë°˜í™˜
    """
    if r["language"] == 'ko':
        return r["comments"]
    else:
        try:
            return r["localizedReview"]["comments"]
        except:
            return None


# ==========================================
#  Airbnb ë¦¬ë·° API ìš”ì²­ í•¨ìˆ˜
# ==========================================
headers = {
    "Content-Type": "application/json",
    "User-Agent": "Mozilla/5.0",
    "Accept": "*/*",
    "Referer": "https://www.airbnb.co.kr/",
    "Accept-Language": "ko-KR,ko;q=0.9",
    "x-airbnb-api-key": "d306zoyjsyarp7ifhu67rjxn52tv0t20",
}

def getReviewsJson(stay_id, limit, offset, headers=headers):
    """
    Airbnb APIë¥¼ í†µí•´ JSON í˜•ì‹ì˜ ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘
    """
    jurl = "https://www.airbnb.co.kr/api/v3/StaysPdpReviewsQuery/dec1c8061483e78373602047450322fd474e79ba9afa8d3dbbc27f504030f91d?"
    variables = {
        "id": stay_id,
        "pdpReviewsRequest": {
            "fieldSelector": "for_p3_translation_only",
            "forPreview": False,
            "limit": limit,
            "offset": str(offset),
            "showingTranslationButton": False,
            "first": limit,
            "sortingPreference": "MOST_RECENT",
            "checkinDate": "2025-06-27",
            "checkoutDate": "2025-07-02",
            "numberOfAdults": "1",
            "numberOfChildren": "0",
            "numberOfInfants": "0",
            "numberOfPets": "0",
            "after": None
        }
    }
    extensions = {
        "persistedQuery": {
            "version": 1,
            "sha256Hash": "dec1c8061483e78373602047450322fd474e79ba9afa8d3dbbc27f504030f91d"
        }
    }
    params = {
        "operationName": "StaysPdpReviewsQuery",
        "locale": "ko",
        "currency": "KRW",
        "variables": json.dumps(variables, separators=(',', ':')),
        "extensions": json.dumps(extensions, separators=(',', ':'))
    }
    resp = requests.get(jurl + urlencode(params, quote_via=quote), headers=headers)
    return resp.json()


# ==========================================
#  ë¦¬ë·° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
# ==========================================
def clean_text(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ìˆ˜ë¬¸ì, íƒœê·¸, ê°œí–‰ë¬¸ì ì œê±° ë° ì •ë¦¬
    """
    text = text.replace('\n', ' ').replace('\r', ' ')
    text = re.sub(r'<br.*?>', ' ', text)
    text = re.sub(r'[^\w\s.,!?ê°€-í£]', '', text)
    text = re.sub(r'([.!?])', r'\1 ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# ==========================================
#  ìˆ™ì†Œ URL ê¸°ë°˜ ë¦¬ë·° ë¶„ì„ ì‹¤í–‰
# ==========================================
def run_topic_model_on_room(room_url):
    """
    1ï¸âƒ£ ìˆ™ì†Œ ID ì¶”ì¶œ ë° ì¸ì½”ë”©
    2ï¸âƒ£ Airbnb APIë¡œ ë¦¬ë·° ìˆ˜ì§‘
    3ï¸âƒ£ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•´ ë° ì „ì²˜ë¦¬
    4ï¸âƒ£ BERTopicìœ¼ë¡œ í† í”½ ëª¨ë¸ë§ ë° ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤í•‘
    5ï¸âƒ£ ì£¼ì œë³„ ë¬¸ì¥ ë¶„ë¥˜ ê²°ê³¼ ë°˜í™˜
    """
    # 1ï¸âƒ£ ìˆ™ì†Œ ID ì¸ì½”ë”©
    p1, _ = room_url.split("?")
    _, num = p1.split("/rooms/")
    num = num.split("/")[0]
    encoding = 'StayListing:' + num
    stay_id = base64.b64encode(encoding.encode('utf-8')).decode('utf-8')

    # 2ï¸âƒ£ ë¦¬ë·° ìˆ˜ì§‘
    data = []
    offset = 0
    limit = 50
    while True:
        res_json = getReviewsJson(stay_id, limit, offset)
        try:
            review_data = res_json["data"]["presentation"]["stayProductDetailPage"]["reviews"]
            reviewsCount = int(review_data['metadata']['reviewsCount'])
            for r in review_data["reviews"]:
                comment_text = comments(r)
                if comment_text:
                    data.append({
                        "user_id": r["reviewer"]["id"],
                        "name": r["reviewer"]["firstName"],
                        "stay_id": num,
                        "comment": comment_text,
                        "rating": r.get("rating"),
                        "createdAt": r.get("createdAt")
                    })
        except:
            break
        if offset >= reviewsCount:
            break
        offset += 50
        limit += 50

    # 3ï¸âƒ£ ë¦¬ë·° DataFrame ìƒì„± ë° ê²€ì¦
    reviews = pd.DataFrame(data).dropna()
    if len(reviews) < 5:
        raise ValueError("ë¦¬ë·°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # 4ï¸âƒ£ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•´
    rows = []
    for j, row in reviews.reset_index(drop=True).iterrows():
        cleaned_comment = clean_text(row['comment'])
        sentences = kss.split_sentences(cleaned_comment)
        idx, split_num = 0, 1
        for sentence in sentences:
            for s in re.split(r'(?<=[ê°€-í£\w])\.(?=[^\d])', sentence.strip()):
                s = s.strip()
                if len(s) < 7: continue
                start = cleaned_comment.find(s, idx)
                end = start + len(s) - 1
                idx = end + 1
                rows.append({
                    "stay_id": num,
                    "user_id": row['user_id'],
                    "splitNum": split_num,
                    "sentence": s,
                    "start": start,
                    "end": end
                })
                split_num += 1

    # 5ï¸âƒ£ ê³ ìœ  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
    sentence_df = pd.DataFrame(rows).dropna()
    docs = sentence_df['sentence'].drop_duplicates().tolist()
    if len(docs) < 5:
        raise ValueError("ë¬¸ì¥ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")

    # 6ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    embedding_model = SentenceTransformer("jhgan/ko-sbert-nli", device=device)

    # 7ï¸âƒ£ ì£¼ì œë³„ ì‹œë“œ í‚¤ì›Œë“œ & ì´ë¦„ ì •ì˜
    seed_topics = [
        ["ì²­ê²°ë„", "ê¹¨ë—í•¨", "ìœ„ìƒ", "ì²­ì†Œ", "ì •ë¦¬", "ê¹”ë”", "ëƒ„ìƒˆ", "ì¾Œì¾Œí•¨", "ê³°íŒ¡ì´ëƒ„ìƒˆ", "í™˜ê¸°"],
        ["ìœ„ì¹˜", "êµí†µ", "í¸ë¦¬í•¨", "ì ‘ê·¼ì„±", "ì¤‘ì•™", "ê·¼ì²˜"],
        ["ê°€ê²©", "ì €ë ´", "ê°€ì„±ë¹„", "ë¹„ìš©", "ê³ ê°€", "ê²½ì œì ", "ìš”ê¸ˆ", "ê°€ì¹˜"],
        ["ì‹œì„¤", "í¸ë¦¬í•¨", "ìƒˆê²ƒ", "êµ¬ë¹„", "ì¥ë¹„", "ê¸°ëŠ¥", "ì„¤ë¹„", "ë‚¡ìŒ"],
        ["í˜¸ìŠ¤íŠ¸", "ì„œë¹„ìŠ¤", "ì¹œì ˆ", "ì‘ëŒ€", "ë„ì›€", "ë¶ˆì¹œì ˆ", "ê´€ë¦¬", "ì²´í¬ì¸", "ì²´í¬ì•„ì›ƒ"],
        ["ì†ŒìŒ", "ì¡°ìš©", "ì‹œë„ëŸ¬ì›€", "ë°©ìŒ", "ì†Œë€", "ë°©í•´"]
    ]
    topic_names = ["ì²­ê²°ë„", "ì†ŒìŒ", "ìœ„ì¹˜", "ê°€ê²©", "ì‹œì„¤", "í˜¸ìŠ¤íŠ¸"]

    # 8ï¸âƒ£ ì°¨ì› ì¶•ì†Œ ë° í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ ì •ì˜
    umap_model = UMAP(n_components=2, random_state=42, metric='cosine', n_neighbors=30, min_dist=0.1)
    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=6, min_samples=3, cluster_selection_method='leaf')

    # 9ï¸âƒ£ BERTopic ëª¨ë¸ í•™ìŠµ
    topic_model = BERTopic(
        embedding_model=embedding_model,
        language="multilingual",
        min_topic_size=6,
        nr_topics="auto",
        seed_topic_list=seed_topics,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        verbose=True,
        vectorizer_model=TfidfVectorizer()
    )
    topics, probs = topic_model.fit_transform(docs)
    topic_info = topic_model.get_topic_info()
    topic_ids = topic_info[topic_info['Topic'] != -1]['Topic'].tolist()

    # ğŸ”Ÿ Cosine Similarityë¡œ í† í”½ ì´ë¦„ ë§¤í•‘
    topic_embeddings = topic_model.topic_embeddings_[1:]
    seed_embeddings = embedding_model.encode([" ".join(keywords) for keywords in seed_topics])
    similarity = cosine_similarity(topic_embeddings, seed_embeddings)
    mapped_topics = {topic_ids[i]: topic_names[sim_row.argmax()] for i, sim_row in enumerate(similarity)}

    # ğŸ”Ÿ ë¬¸ì¥ë“¤ì„ ì£¼ì œë³„ë¡œ ë¶„ë¥˜
    topic_sentences = defaultdict(list)
    for doc, topic_id in zip(docs, topics):
        label = mapped_topics.get(topic_id, "ê¸°íƒ€")
        if label != "ê¸°íƒ€":
            topic_sentences[label].append(doc)

    # ğŸ”Ÿ ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return {"stay_id": num, "topics": topic_sentences}
