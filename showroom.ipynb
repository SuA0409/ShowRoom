{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"rTi21Ty3WZI4","outputId":"605ef62d-7d34-44d8-e2a0-33c9d9b0efc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Model/ShowRoom\n","Collecting lightning>=2.5.0 (from -r requirements.txt (line 2))\n","  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n","Collecting lightning-bolts (from -r requirements.txt (line 3))\n","  Downloading lightning_bolts-0.7.0-py3-none-any.whl.metadata (9.5 kB)\n","Collecting lightning-utilities (from -r requirements.txt (line 4))\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Collecting torchmetrics (from -r requirements.txt (line 5))\n","  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n","Collecting torchinfo (from -r requirements.txt (line 6))\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Collecting hydra-core (from -r requirements.txt (line 9))\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting hydra-colorlog (from -r requirements.txt (line 10))\n","  Downloading hydra_colorlog-1.2.0-py3-none-any.whl.metadata (949 bytes)\n","Collecting hydra-optuna-sweeper (from -r requirements.txt (line 11))\n","  Downloading hydra_optuna_sweeper-1.2.0-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.19.11)\n","Collecting rootutils (from -r requirements.txt (line 17))\n","  Downloading rootutils-1.0.7-py3-none-any.whl.metadata (4.7 kB)\n","Collecting pre-commit (from -r requirements.txt (line 18))\n","  Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (13.9.4)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (8.3.5)\n","Collecting nvitop (from -r requirements.txt (line 21))\n","  Downloading nvitop-1.5.0-py3-none-any.whl.metadata (80 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy<2.0.0 (from -r requirements.txt (line 24))\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trimesh (from -r requirements.txt (line 25))\n","  Downloading trimesh-4.6.9-py3-none-any.whl.metadata (18 kB)\n","Collecting roma (from -r requirements.txt (line 26))\n","  Downloading roma-1.5.2.1-py3-none-any.whl.metadata (5.4 kB)\n","Collecting open3d (from -r requirements.txt (line 27))\n","  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.8.1)\n","Collecting deepspeed (from -r requirements.txt (line 29))\n","  Downloading deepspeed-0.16.7.tar.gz (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio (from -r requirements.txt (line 30))\n","  Downloading gradio-5.29.1-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (3.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (4.67.1)\n","Collecting jupyterlab (from -r requirements.txt (line 33))\n","  Downloading jupyterlab-4.4.2-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (4.11.0.86)\n","Collecting viser (from -r requirements.txt (line 35))\n","  Downloading viser-0.2.23-py3-none-any.whl.metadata (4.8 kB)\n","Collecting pillow_heif (from -r requirements.txt (line 36))\n","  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (5.24.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (0.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (1.15.3)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (0.13.2)\n","Collecting pyglet<2 (from -r requirements.txt (line 42))\n","  Downloading pyglet-1.5.31-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: huggingface-hub>=0.22 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[torch]>=0.22->-r requirements.txt (line 43)) (0.31.2)\n","Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.5.0->-r requirements.txt (line 2)) (6.0.2)\n","Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (2025.3.2)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.5.0->-r requirements.txt (line 2)) (24.2)\n","Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.5.0->-r requirements.txt (line 2)) (2.6.0+cu124)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.5.0->-r requirements.txt (line 2)) (4.13.2)\n","Collecting pytorch-lightning (from lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n","  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning-bolts->-r requirements.txt (line 3)) (0.21.0+cu124)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from lightning-bolts->-r requirements.txt (line 3)) (2.18.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities->-r requirements.txt (line 4)) (75.2.0)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from hydra-core->-r requirements.txt (line 9)) (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core->-r requirements.txt (line 9)) (4.9.3)\n","Collecting colorlog (from hydra-colorlog->-r requirements.txt (line 10))\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Collecting hydra-core (from -r requirements.txt (line 9))\n","  Downloading hydra_core-1.4.0.dev1-py3-none-any.whl.metadata (5.4 kB)\n","Collecting optuna<3.0.0,>=2.10.0 (from hydra-optuna-sweeper->-r requirements.txt (line 11))\n","  Downloading optuna-2.10.1-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (8.2.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (4.3.8)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (5.29.4)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (5.9.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (2.11.4)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (2.28.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 14)) (1.3.6)\n","Collecting python-dotenv>=0.20.0 (from rootutils->-r requirements.txt (line 17))\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Collecting cfgv>=2.0.0 (from pre-commit->-r requirements.txt (line 18))\n","  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n","Collecting identify>=1.0.0 (from pre-commit->-r requirements.txt (line 18))\n","  Downloading identify-2.6.10-py2.py3-none-any.whl.metadata (4.4 kB)\n","Collecting nodeenv>=0.11.1 (from pre-commit->-r requirements.txt (line 18))\n","  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n","Collecting virtualenv>=20.10.0 (from pre-commit->-r requirements.txt (line 18))\n","  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 19)) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 19)) (2.19.1)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->-r requirements.txt (line 20)) (2.1.0)\n","Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->-r requirements.txt (line 20)) (1.5.0)\n","Collecting nvidia-ml-py<12.571.0a0,>=11.450.51 (from nvitop->-r requirements.txt (line 21))\n","  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n","Collecting dash>=2.6.0 (from open3d->-r requirements.txt (line 27))\n","  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d->-r requirements.txt (line 27)) (3.1.3)\n","Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d->-r requirements.txt (line 27)) (3.1.1)\n","Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d->-r requirements.txt (line 27)) (5.10.4)\n","Collecting configargparse (from open3d->-r requirements.txt (line 27))\n","  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n","Collecting ipywidgets>=8.0.4 (from open3d->-r requirements.txt (line 27))\n","  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n","Collecting addict (from open3d->-r requirements.txt (line 27))\n","  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d->-r requirements.txt (line 27)) (11.2.1)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d->-r requirements.txt (line 27)) (2.2.2)\n","Collecting pyquaternion (from open3d->-r requirements.txt (line 27))\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n","Collecting hjson (from deepspeed->-r requirements.txt (line 29))\n","  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed->-r requirements.txt (line 29)) (1.1.0)\n","Collecting ninja (from deepspeed->-r requirements.txt (line 29))\n","  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed->-r requirements.txt (line 29)) (9.0.0)\n","Collecting aiofiles<25.0,>=22.0 (from gradio->-r requirements.txt (line 30))\n","  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 30)) (4.9.0)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio->-r requirements.txt (line 30))\n","  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio->-r requirements.txt (line 30))\n","  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n","Collecting gradio-client==1.10.1 (from gradio->-r requirements.txt (line 30))\n","  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting groovy~=0.1 (from gradio->-r requirements.txt (line 30))\n","  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 30)) (0.28.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 30)) (3.1.6)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 30)) (3.0.2)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 30)) (3.10.18)\n","Collecting pydub (from gradio->-r requirements.txt (line 30))\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.18 (from gradio->-r requirements.txt (line 30))\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Collecting ruff>=0.9.3 (from gradio->-r requirements.txt (line 30))\n","  Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting safehttpx<0.2.0,>=0.1.6 (from gradio->-r requirements.txt (line 30))\n","  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n","Collecting semantic-version~=2.0 (from gradio->-r requirements.txt (line 30))\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio->-r requirements.txt (line 30))\n","  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n","Collecting tomlkit<0.14.0,>=0.12.0 (from gradio->-r requirements.txt (line 30))\n","  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 30)) (0.15.3)\n","Collecting uvicorn>=0.14.0 (from gradio->-r requirements.txt (line 30))\n","  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio->-r requirements.txt (line 30)) (15.0.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 31)) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 31)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 31)) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 31)) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 31)) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 31)) (2.9.0.post0)\n","Collecting async-lru>=1.0.0 (from jupyterlab->-r requirements.txt (line 33))\n","  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: ipykernel>=6.5.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 33)) (6.17.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 33)) (5.7.2)\n","Collecting jupyter-lsp>=2.0.0 (from jupyterlab->-r requirements.txt (line 33))\n","  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n","Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->-r requirements.txt (line 33))\n","  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->-r requirements.txt (line 33))\n","  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 33)) (0.2.4)\n","Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 33)) (6.4.2)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from jupyterlab->-r requirements.txt (line 33)) (5.7.1)\n","Requirement already satisfied: imageio>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from viser->-r requirements.txt (line 35)) (2.37.0)\n","Collecting msgspec>=0.18.6 (from viser->-r requirements.txt (line 35))\n","  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Collecting plyfile>=1.0.2 (from viser->-r requirements.txt (line 35))\n","  Downloading plyfile-1.1-py3-none-any.whl.metadata (2.1 kB)\n","Collecting tyro>=0.2.0 (from viser->-r requirements.txt (line 35))\n","  Downloading tyro-0.9.20-py3-none-any.whl.metadata (10 kB)\n","Collecting yourdfpy>=0.0.53 (from viser->-r requirements.txt (line 35))\n","  Downloading yourdfpy-0.0.57-py3-none-any.whl.metadata (9.1 kB)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->-r requirements.txt (line 37)) (9.1.2)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 38)) (3.4.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 38)) (2025.5.10)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 38)) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 39)) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 39)) (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.22->huggingface-hub[torch]>=0.22->-r requirements.txt (line 43)) (3.18.0)\n","Requirement already satisfied: safetensors[torch] in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[torch]>=0.22->-r requirements.txt (line 43)) (0.5.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 30)) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 30)) (1.3.1)\n","Collecting flask>=3.0.0 (from open3d->-r requirements.txt (line 27))\n","  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n","Collecting werkzeug>=3.0.0 (from open3d->-r requirements.txt (line 27))\n","  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 27)) (8.7.0)\n","Collecting retrying (from dash>=2.6.0->open3d->-r requirements.txt (line 27))\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d->-r requirements.txt (line 27)) (1.6.0)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 14)) (1.17.0)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d->-r requirements.txt (line 27)) (2.2.0)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d->-r requirements.txt (line 27)) (1.9.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (3.11.15)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 14)) (4.0.12)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 30)) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 30)) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r requirements.txt (line 30)) (0.16.0)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (1.8.0)\n","Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (7.34.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (6.1.12)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (0.1.7)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (24.0.1)\n","Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 27))\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 27))\n","  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d->-r requirements.txt (line 27)) (3.0.15)\n","Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (23.1.0)\n","Collecting jupyter-client>=6.1.12 (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n","Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n","Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (7.16.6)\n","Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (0.21.1)\n","Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (0.18.1)\n","Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (1.8.0)\n","Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 33)) (2.17.0)\n","Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 33))\n","  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n","Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 33)) (4.23.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 19)) (0.1.2)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d->-r requirements.txt (line 27)) (2.21.1)\n","Collecting alembic (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11))\n","  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n","Collecting cliff (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11))\n","  Downloading cliff-4.10.0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting cmaes>=0.8.2 (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11))\n","  Downloading cmaes-0.11.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11)) (2.0.40)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d->-r requirements.txt (line 27)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d->-r requirements.txt (line 27)) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 14)) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 14)) (2.4.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->lightning-bolts->-r requirements.txt (line 3)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->lightning-bolts->-r requirements.txt (line 3)) (1.71.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->lightning-bolts->-r requirements.txt (line 3)) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->lightning-bolts->-r requirements.txt (line 3)) (0.7.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2)) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2)) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2)) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2))\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2)) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning>=2.5.0->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 30)) (1.5.4)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.2.0->viser->-r requirements.txt (line 35)) (0.16)\n","Collecting shtab>=1.5.6 (from tyro>=0.2.0->viser->-r requirements.txt (line 35))\n","  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.2.0->viser->-r requirements.txt (line 35)) (4.4.2)\n","Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->-r requirements.txt (line 18))\n","  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from yourdfpy>=0.0.53->viser->-r requirements.txt (line 35)) (5.4.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.5.0->-r requirements.txt (line 2)) (1.20.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (21.2.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 14)) (5.0.2)\n","Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (3.0.51)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (4.9.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 33)) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 33)) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 33)) (0.24.0)\n","Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n","Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (4.13.4)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (3.1.3)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (0.10.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (1.5.1)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11)) (3.2.2)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (0.7.0)\n","Collecting manifold3d>=2.3.0 (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35))\n","  Downloading manifold3d-3.1.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (18 kB)\n","Collecting svg.path (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35))\n","  Downloading svg.path-6.3-py2.py3-none-any.whl.metadata (13 kB)\n","Collecting pycollada (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35))\n","  Downloading pycollada-0.9.tar.gz (109 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35)) (2.1.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35)) (3.5.0)\n","Collecting rtree (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35))\n","  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n","Collecting embreex (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35))\n","  Downloading embreex-2.17.7.post6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.9 kB)\n","Collecting vhacdx (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35))\n","  Downloading vhacdx-0.0.8.post2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n","Collecting mapbox_earcut>=1.0.2 (from trimesh[easy]>=3.11.2->yourdfpy>=0.0.53->viser->-r requirements.txt (line 35))\n","  Downloading mapbox_earcut-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11)) (1.1.3)\n","Collecting autopage>=0.4.0 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11))\n","  Downloading autopage-0.5.2-py3-none-any.whl.metadata (7.9 kB)\n","Collecting cmd2>=1.0.0 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11))\n","  Downloading cmd2-2.5.11-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11)) (3.16.0)\n","Collecting stevedore>=2.0.1 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11))\n","  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d->-r requirements.txt (line 27)) (3.21.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (1.4.0)\n","Requirement already satisfied: pyperclip>=1.8 in /usr/local/lib/python3.11/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11)) (1.9.0)\n","Requirement already satisfied: wcwidth>=0.2.10 in /usr/local/lib/python3.11/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11)) (0.2.13)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 33)) (0.8.4)\n","Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (3.0.0)\n","Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (24.11.1)\n","Collecting pbr>=2.0.0 (from stevedore>=2.0.1->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r requirements.txt (line 11))\n","  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (2.7)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33)) (2.22)\n","Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n","Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 33))\n","  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n","Downloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_colorlog-1.2.0-py3-none-any.whl (3.6 kB)\n","Downloading hydra_optuna_sweeper-1.2.0-py3-none-any.whl (8.5 kB)\n","Downloading rootutils-1.0.7-py3-none-any.whl (6.4 kB)\n","Downloading pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvitop-1.5.0-py3-none-any.whl (213 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.8/213.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trimesh-4.6.9-py3-none-any.whl (711 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m711.1/711.1 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading roma-1.5.2.1-py3-none-any.whl (24 kB)\n","Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio-5.29.1-py3-none-any.whl (54.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyterlab-4.4.2-py3-none-any.whl (12.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m130.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading viser-0.2.23-py3-none-any.whl (28.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.9/28.9 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m139.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyglet-1.5.31-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n","Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n","Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n","Downloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n","Downloading identify-2.6.10-py2.py3-none-any.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n","Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading plyfile-1.1-py3-none-any.whl (23 kB)\n","Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Downloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m142.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.20-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m136.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yourdfpy-0.0.57-py3-none-any.whl (22 kB)\n","Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n","Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n","Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Downloading cmaes-0.11.1-py3-none-any.whl (35 kB)\n","Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n","Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n","Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n","Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n","Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cliff-4.10.0-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Downloading autopage-0.5.2-py3-none-any.whl (30 kB)\n","Downloading cmd2-2.5.11-py3-none-any.whl (152 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading manifold3d-3.1.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mapbox_earcut-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.0/97.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n","Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n","Downloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading embreex-2.17.7.post6-cp311-cp311-manylinux_2_28_x86_64.whl (17.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading svg.path-6.3-py2.py3-none-any.whl (16 kB)\n","Downloading vhacdx-0.0.8.post2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n","Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: deepspeed, pycollada\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.16.7-py3-none-any.whl size=1642800 sha256=56801609fd79ed6629e341b8266275b22761454f9b82e4418e2c7ff62ad199a6\n","  Stored in directory: /root/.cache/pip/wheels/42/e7/1a/2106f7197cc13e09c68f1b4f55f7e5117a985e726378968970\n","  Building wheel for pycollada (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycollada: filename=pycollada-0.9-py3-none-any.whl size=128523 sha256=850d26e83d040cb73d20a024fb1b3e2a4d6e3fb719a358f93b5c2c11bf1d549b\n","  Stored in directory: /root/.cache/pip/wheels/d8/f5/25/96914138081f519b3061d82b1306fccf17ea00e00a4b3f8fb2\n","Successfully built deepspeed pycollada\n","Installing collected packages: pyglet, pydub, nvidia-ml-py, hjson, distlib, addict, widgetsnbextension, werkzeug, virtualenv, uvicorn, uri-template, types-python-dateutil, torchinfo, tomlkit, svg.path, shtab, semantic-version, ruff, rtree, roma, rfc3986-validator, rfc3339-validator, retrying, python-multipart, python-json-logger, python-dotenv, pillow_heif, pbr, overrides, nvitop, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nodeenv, ninja, msgspec, lightning-utilities, json5, jedi, identify, groovy, fqdn, ffmpy, configargparse, comm, colorlog, cmd2, cfgv, autopage, async-lru, aiofiles, vhacdx, trimesh, stevedore, starlette, rootutils, pyquaternion, pycollada, pre-commit, plyfile, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mapbox_earcut, manifold3d, jupyter-server-terminals, jupyter-client, hydra-core, flask, embreex, cmaes, arrow, alembic, tyro, safehttpx, nvidia-cusolver-cu12, isoduration, ipywidgets, hydra-colorlog, gradio-client, fastapi, dash, cliff, optuna, gradio, yourdfpy, torchmetrics, open3d, jupyter-events, hydra-optuna-sweeper, deepspeed, viser, pytorch-lightning, lightning-bolts, lightning, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab\n","  Attempting uninstall: nvidia-ml-py\n","    Found existing installation: nvidia-ml-py 12.575.51\n","    Uninstalling nvidia-ml-py-12.575.51:\n","      Successfully uninstalled nvidia-ml-py-12.575.51\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.10\n","    Uninstalling widgetsnbextension-3.6.10:\n","      Successfully uninstalled widgetsnbextension-3.6.10\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 3.1.3\n","    Uninstalling Werkzeug-3.1.3:\n","      Successfully uninstalled Werkzeug-3.1.3\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: jupyter-client\n","    Found existing installation: jupyter-client 6.1.12\n","    Uninstalling jupyter-client-6.1.12:\n","      Successfully uninstalled jupyter-client-6.1.12\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 3.1.1\n","    Uninstalling Flask-3.1.1:\n","      Successfully uninstalled Flask-3.1.1\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","  Attempting uninstall: jupyter-server\n","    Found existing installation: jupyter-server 1.16.0\n","    Uninstalling jupyter-server-1.16.0:\n","      Successfully uninstalled jupyter-server-1.16.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","notebook 6.5.7 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\n","jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed addict-2.4.0 aiofiles-24.1.0 alembic-1.15.2 arrow-1.3.0 async-lru-2.0.5 autopage-0.5.2 cfgv-3.4.0 cliff-4.10.0 cmaes-0.11.1 cmd2-2.5.11 colorlog-6.9.0 comm-0.2.2 configargparse-1.7 dash-3.0.4 deepspeed-0.16.7 distlib-0.3.9 embreex-2.17.7.post6 fastapi-0.115.12 ffmpy-0.5.0 flask-3.0.3 fqdn-1.5.1 gradio-5.29.1 gradio-client-1.10.1 groovy-0.1.2 hjson-3.1.0 hydra-colorlog-1.2.0 hydra-core-1.3.2 hydra-optuna-sweeper-1.2.0 identify-2.6.10 ipywidgets-8.1.7 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-client-8.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.2 jupyterlab-server-2.27.3 lightning-2.5.1.post0 lightning-bolts-0.7.0 lightning-utilities-0.14.3 manifold3d-3.1.0 mapbox_earcut-1.0.3 msgspec-0.19.0 ninja-1.11.1.4 nodeenv-1.9.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-ml-py-12.570.86 nvidia-nvjitlink-cu12-12.4.127 nvitop-1.5.0 open3d-0.19.0 optuna-2.10.1 overrides-7.7.0 pbr-6.1.1 pillow_heif-0.22.0 plyfile-1.1 pre-commit-4.2.0 pycollada-0.9 pydub-0.25.1 pyglet-1.5.31 pyquaternion-0.9.9 python-dotenv-1.1.0 python-json-logger-3.3.0 python-multipart-0.0.20 pytorch-lightning-1.9.5 retrying-1.3.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 roma-1.5.2.1 rootutils-1.0.7 rtree-1.4.0 ruff-0.11.10 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.2 starlette-0.46.2 stevedore-5.4.1 svg.path-6.3 tomlkit-0.13.2 torchinfo-1.8.0 torchmetrics-1.7.1 trimesh-4.6.9 types-python-dateutil-2.9.0.20250516 tyro-0.9.20 uri-template-1.3.0 uvicorn-0.34.2 vhacdx-0.0.8.post2 virtualenv-20.31.2 viser-0.2.23 werkzeug-3.0.6 widgetsnbextension-4.0.14 yourdfpy-0.0.57\n","Collecting numpy==2.2.5\n","  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m346.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.2.5\n"]}],"source":["## 사전 설치\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/Colab Notebooks/Model/ShowRoom\n","!pip install -r requirements.txt\n","\n","!pip install --upgrade --no-cache-dir --force-reinstall numpy==2.2.5\n","import os\n","os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":6490,"status":"ok","timestamp":1747435972822,"user":{"displayName":"지능정보 SW아카데미5조","userId":"16997830524223997531"},"user_tz":-540},"id":"ytK3eRzf9DJA","outputId":"762c90b2-ac30-4bbc-dbde-3acb77479cbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Model/ShowRoom\n","Obtaining file:///content/drive/MyDrive/Colab%20Notebooks/Model/ShowRoom\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Installing collected packages: fast3r\n","  Attempting uninstall: fast3r\n","    Found existing installation: fast3r 1.0\n","    Uninstalling fast3r-1.0:\n","      Successfully uninstalled fast3r-1.0\n","  Running setup.py develop for fast3r\n","Successfully installed fast3r-1.0\n"]}],"source":["# load fast3r git clone\n","%cd /content/drive/MyDrive/Colab Notebooks/Model/ShowRoom\n","!pip install -e ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"collapsed":true,"executionInfo":{"elapsed":9525,"status":"ok","timestamp":1747435982391,"user":{"displayName":"지능정보 SW아카데미5조","userId":"16997830524223997531"},"user_tz":-540},"id":"wADGej9b-Nlu","outputId":"7505fc63-cd0e-42df-ac01-707b896d4293"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Model/ShowRoom/s3r\n","Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n","  if not hasattr(numpy, tp_name):\n","/usr/local/lib/python3.11/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n","  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n","/usr/local/lib/python3.11/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n","  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n","/usr/local/lib/python3.11/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n","  self.nce_loss = AmdimNCELoss(tclip)\n"]}],"source":["## import\n","%cd s3r\n","import torch\n","import os\n","import time\n","import re\n","import json\n","import copy\n","\n","from fast3r.dust3r.inference_multiview import inference\n","from fast3r.models.fast3r import Fast3R\n","from fast3r.models.multiview_dust3r_module import MultiViewDUSt3RLitModule\n","\n","from show_loss import RKDLoss\n","from room2images_2 import batch_images_load\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amh824unq7yp"},"outputs":[],"source":["with open('/content/drive/MyDrive/Colab Notebooks/Model/ShowRoom/configs/teacher_args.json', 'r') as f:\n","    teacher_args = json.load(f)\n","teacher_args['head_args']['conf_mode']=['exp', 1, float('inf')]\n","teacher_args['head_args']['depth_mode']= ['exp', float('-inf'), float('inf')]\n","student_args = copy.deepcopy(teacher_args)\n","\n","## 수정 파라미터\n","# student_args['head_args']['layer_dims'] = [48, 96, 192, 384]\n","student_args['head_args']['feature_dim'] = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQm4YcKLeMW8"},"outputs":[],"source":["# 1 번재 테스트 : \"layer_dims\":[48, 96, 192, 384], \"feature_dim\":128, <- half head"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"zJlXxF-pw8fG","outputId":"c04900c7-b582-4a4a-dda4-75d8064c642b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Train Data : 9000\n","device : cuda \n","\n","Building Teacher model ...\n","Finished Building Teacher model ! \n","\n","Building Student model ...\n","Finished Building Student model ! \n","\n","loaded : Teacher's en/decoder \n","\n","########### START epoch : 1 ###########\n","Data Loading\n","data_torch-4000.pt Data Loading Time : 157.551\n","epoch : 1    batch :    40    loss : 0.07227    time : 165.276\n","epoch : 1    batch :    80    loss : 0.01616    time :  5.744\n","epoch : 1    batch :   120    loss : 0.01750    time :  5.719\n","epoch : 1    batch :   160    loss : 0.01998    time :  5.732\n","epoch : 1    batch :   200    loss : 0.02850    time :  5.745\n","epoch : 1    batch :   240    loss : 0.01257    time :  5.738\n","epoch : 1    batch :   280    loss : 0.02972    time :  5.748\n","epoch : 1    batch :   320    loss : 0.01630    time :  5.756\n","epoch : 1    batch :   360    loss : 0.02640    time :  5.742\n","epoch : 1    batch :   400    loss : 0.00819    time :  5.780\n","epoch : 1    batch :   440    loss : 0.00587    time :  5.728\n","epoch : 1    batch :   480    loss : 0.00828    time :  5.758\n","epoch : 1    batch :   520    loss : 0.00981    time :  5.748\n","epoch : 1    batch :   560    loss : 0.00105    time :  5.767\n","epoch : 1    batch :   600    loss : 0.00317    time :  5.737\n","epoch : 1    batch :   640    loss : 0.00138    time :  5.754\n","epoch : 1    batch :   680    loss : 0.00563    time :  5.744\n","epoch : 1    batch :   720    loss : 0.00374    time :  5.772\n","epoch : 1    batch :   760    loss : 0.00321    time :  5.729\n","epoch : 1    batch :   800    loss : 0.00232    time :  5.783\n","epoch : 1    batch :   840    loss : 0.00987    time :  5.741\n","epoch : 1    batch :   880    loss : 0.00969    time :  5.771\n","epoch : 1    batch :   920    loss : 0.01079    time :  5.778\n","epoch : 1    batch :   960    loss : 0.00885    time :  5.785\n","epoch : 1    batch :  1000    loss : 0.01484    time :  5.752\n","epoch : 1    batch :  1040    loss : 0.00677    time :  5.758\n","epoch : 1    batch :  1080    loss : 0.01101    time :  5.771\n","epoch : 1    batch :  1120    loss : 0.02235    time :  5.782\n","epoch : 1    batch :  1160    loss : 0.00363    time :  5.783\n","epoch : 1    batch :  1200    loss : 0.00484    time :  5.780\n","epoch : 1    batch :  1240    loss : 0.00246    time :  5.778\n","epoch : 1    batch :  1280    loss : 0.00318    time :  5.767\n","epoch : 1    batch :  1320    loss : 0.01075    time :  5.791\n","epoch : 1    batch :  1360    loss : 0.00235    time :  5.764\n","epoch : 1    batch :  1400    loss : 0.00180    time :  5.793\n","epoch : 1    batch :  1440    loss : 0.00656    time :  5.732\n","epoch : 1    batch :  1480    loss : 0.00514    time :  5.785\n","epoch : 1    batch :  1520    loss : 0.00701    time :  5.754\n","epoch : 1    batch :  1560    loss : 0.00222    time :  5.777\n","epoch : 1    batch :  1600    loss : 0.00318    time :  5.738\n","epoch : 1    batch :  1640    loss : 0.00496    time :  5.761\n","epoch : 1    batch :  1680    loss : 0.00978    time :  5.738\n","epoch : 1    batch :  1720    loss : 0.01112    time :  5.723\n","epoch : 1    batch :  1760    loss : 0.01791    time :  5.734\n","epoch : 1    batch :  1800    loss : 0.00188    time :  5.739\n","epoch : 1    batch :  1840    loss : 0.00229    time :  5.717\n","epoch : 1    batch :  1880    loss : 0.00178    time :  5.744\n","epoch : 1    batch :  1920    loss : 0.00355    time :  5.711\n","epoch : 1    batch :  1960    loss : 0.00089    time :  5.734\n","epoch : 1    batch :  2000    loss : 0.00429    time :  5.727\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch :  2040    loss : 0.00366    time : 11.434\n","epoch : 1    batch :  2080    loss : 0.00139    time :  5.796\n","epoch : 1    batch :  2120    loss : 0.00265    time :  5.767\n","epoch : 1    batch :  2160    loss : 0.00288    time :  5.860\n","epoch : 1    batch :  2200    loss : 0.00066    time :  5.756\n","epoch : 1    batch :  2240    loss : 0.00047    time :  5.761\n","epoch : 1    batch :  2280    loss : 0.00642    time :  5.800\n","epoch : 1    batch :  2320    loss : 0.00282    time :  5.847\n","epoch : 1    batch :  2360    loss : 0.00482    time :  5.840\n","epoch : 1    batch :  2400    loss : 0.00750    time :  5.824\n","epoch : 1    batch :  2440    loss : 0.01280    time :  5.897\n","epoch : 1    batch :  2480    loss : 0.01056    time :  5.730\n","epoch : 1    batch :  2520    loss : 0.01245    time :  5.779\n","epoch : 1    batch :  2560    loss : 0.00121    time :  5.748\n","epoch : 1    batch :  2600    loss : 0.00578    time :  5.754\n","epoch : 1    batch :  2640    loss : 0.00347    time :  5.750\n","epoch : 1    batch :  2680    loss : 0.00030    time :  6.042\n","epoch : 1    batch :  2720    loss : 0.00291    time :  5.739\n","epoch : 1    batch :  2760    loss : 0.00204    time :  5.738\n","epoch : 1    batch :  2800    loss : 0.00251    time :  5.725\n","epoch : 1    batch :  2840    loss : 0.00080    time :  5.768\n","epoch : 1    batch :  2880    loss : 0.00236    time :  5.739\n","epoch : 1    batch :  2920    loss : 0.00069    time :  5.761\n","epoch : 1    batch :  2960    loss : 0.00077    time :  5.727\n","epoch : 1    batch :  3000    loss : 0.00108    time :  5.760\n","epoch : 1    batch :  3040    loss : 0.00343    time :  5.751\n","epoch : 1    batch :  3080    loss : 0.00292    time :  5.741\n","epoch : 1    batch :  3120    loss : 0.00028    time :  5.737\n","epoch : 1    batch :  3160    loss : 0.00208    time :  5.758\n","epoch : 1    batch :  3200    loss : 0.00091    time :  5.732\n","epoch : 1    batch :  3240    loss : 0.00286    time :  5.756\n","epoch : 1    batch :  3280    loss : 0.00161    time :  5.716\n","epoch : 1    batch :  3320    loss : 0.00095    time :  5.740\n","epoch : 1    batch :  3360    loss : 0.00256    time :  5.725\n","epoch : 1    batch :  3400    loss : 0.00116    time :  5.736\n","epoch : 1    batch :  3440    loss : 0.00185    time :  5.740\n","epoch : 1    batch :  3480    loss : 0.01013    time :  5.755\n","epoch : 1    batch :  3520    loss : 0.00284    time :  5.737\n","epoch : 1    batch :  3560    loss : 0.00200    time :  5.754\n","epoch : 1    batch :  3600    loss : 0.00202    time :  5.705\n","epoch : 1    batch :  3640    loss : 0.00416    time :  5.741\n","epoch : 1    batch :  3680    loss : 0.00158    time :  5.713\n","epoch : 1    batch :  3720    loss : 0.00305    time :  5.738\n","epoch : 1    batch :  3760    loss : 0.00334    time :  5.738\n","epoch : 1    batch :  3800    loss : 0.00388    time :  5.718\n","epoch : 1    batch :  3840    loss : 0.00368    time :  5.760\n","epoch : 1    batch :  3880    loss : 0.00205    time :  5.715\n","epoch : 1    batch :  3920    loss : 0.00166    time :  5.743\n","epoch : 1    batch :  3960    loss : 0.00110    time :  5.700\n","epoch : 1    batch :  4000    loss : 0.00632    time :  5.737\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-8000.pt Data Loading Time : 142.032\n","epoch : 1    batch :  4040    loss : 0.00272    time : 152.929\n","epoch : 1    batch :  4080    loss : 0.00306    time :  5.719\n","epoch : 1    batch :  4120    loss : 0.00145    time :  5.727\n","epoch : 1    batch :  4160    loss : 0.00133    time :  5.710\n","epoch : 1    batch :  4200    loss : 0.00177    time :  5.737\n","epoch : 1    batch :  4240    loss : 0.00173    time :  5.707\n","epoch : 1    batch :  4280    loss : 0.00200    time :  5.731\n","epoch : 1    batch :  4320    loss : 0.00511    time :  5.723\n","epoch : 1    batch :  4360    loss : 0.00162    time :  5.749\n","epoch : 1    batch :  4400    loss : 0.00177    time :  5.758\n","epoch : 1    batch :  4440    loss : 0.00303    time :  5.769\n","epoch : 1    batch :  4480    loss : 0.00058    time :  5.752\n","epoch : 1    batch :  4520    loss : 0.00282    time :  5.753\n","epoch : 1    batch :  4560    loss : 0.00233    time :  5.746\n","epoch : 1    batch :  4600    loss : 0.00146    time :  5.759\n","epoch : 1    batch :  4640    loss : 0.00073    time :  5.744\n","epoch : 1    batch :  4680    loss : 0.00058    time :  5.769\n","epoch : 1    batch :  4720    loss : 0.00044    time :  5.762\n","epoch : 1    batch :  4760    loss : 0.00053    time :  5.766\n","epoch : 1    batch :  4800    loss : 0.00048    time :  5.778\n","epoch : 1    batch :  4840    loss : 0.00080    time :  5.759\n","epoch : 1    batch :  4880    loss : 0.00172    time :  5.760\n","epoch : 1    batch :  4920    loss : 0.00242    time :  5.754\n","epoch : 1    batch :  4960    loss : 0.00069    time :  5.781\n","epoch : 1    batch :  5000    loss : 0.00101    time :  5.772\n","epoch : 1    batch :  5040    loss : 0.00188    time :  5.782\n","epoch : 1    batch :  5080    loss : 0.00117    time :  5.789\n","epoch : 1    batch :  5120    loss : 0.00229    time :  5.770\n","epoch : 1    batch :  5160    loss : 0.00793    time :  5.760\n","epoch : 1    batch :  5200    loss : 0.00399    time :  5.753\n","epoch : 1    batch :  5240    loss : 0.00291    time :  5.751\n","epoch : 1    batch :  5280    loss : 0.00045    time :  5.985\n","epoch : 1    batch :  5320    loss : 0.00112    time :  5.703\n","epoch : 1    batch :  5360    loss : 0.00113    time :  5.728\n","epoch : 1    batch :  5400    loss : 0.00033    time :  5.725\n","epoch : 1    batch :  5440    loss : 0.00105    time :  5.728\n","epoch : 1    batch :  5480    loss : 0.00217    time :  5.752\n","epoch : 1    batch :  5520    loss : 0.00066    time :  5.742\n","epoch : 1    batch :  5560    loss : 0.00044    time :  5.734\n","epoch : 1    batch :  5600    loss : 0.00012    time :  5.746\n","epoch : 1    batch :  5640    loss : 0.00709    time :  5.718\n","epoch : 1    batch :  5680    loss : 0.00041    time :  5.738\n","epoch : 1    batch :  5720    loss : 0.00861    time :  5.720\n","epoch : 1    batch :  5760    loss : 0.00297    time :  5.727\n","epoch : 1    batch :  5800    loss : 0.00106    time :  5.707\n","epoch : 1    batch :  5840    loss : 0.00041    time :  5.734\n","epoch : 1    batch :  5880    loss : 0.00030    time :  5.732\n","epoch : 1    batch :  5920    loss : 0.00035    time :  5.730\n","epoch : 1    batch :  5960    loss : 0.00258    time :  5.740\n","epoch : 1    batch :  6000    loss : 0.00081    time :  5.756\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch :  6040    loss : 0.00033    time : 11.503\n","epoch : 1    batch :  6080    loss : 0.00121    time :  5.745\n","epoch : 1    batch :  6120    loss : 0.00084    time :  5.770\n","epoch : 1    batch :  6160    loss : 0.00137    time :  5.775\n","epoch : 1    batch :  6200    loss : 0.00176    time :  5.885\n","epoch : 1    batch :  6240    loss : 0.00043    time :  5.788\n","epoch : 1    batch :  6280    loss : 0.00143    time :  5.809\n","epoch : 1    batch :  6320    loss : 0.00132    time :  5.730\n","epoch : 1    batch :  6360    loss : 0.00034    time :  5.782\n","epoch : 1    batch :  6400    loss : 0.00280    time :  5.735\n","epoch : 1    batch :  6440    loss : 0.00030    time :  5.741\n","epoch : 1    batch :  6480    loss : 0.00070    time :  5.724\n","epoch : 1    batch :  6520    loss : 0.00038    time :  5.740\n","epoch : 1    batch :  6560    loss : 0.00212    time :  5.717\n","epoch : 1    batch :  6600    loss : 0.00062    time :  5.730\n","epoch : 1    batch :  6640    loss : 0.00126    time :  5.740\n","epoch : 1    batch :  6680    loss : 0.00028    time :  5.714\n","epoch : 1    batch :  6720    loss : 0.00205    time :  5.743\n","epoch : 1    batch :  6760    loss : 0.00482    time :  5.724\n","epoch : 1    batch :  6800    loss : 0.00832    time :  5.723\n","epoch : 1    batch :  6840    loss : 0.00250    time :  5.707\n","epoch : 1    batch :  6880    loss : 0.00225    time :  5.715\n","epoch : 1    batch :  6920    loss : 0.00731    time :  5.725\n","epoch : 1    batch :  6960    loss : 0.00532    time :  5.729\n","epoch : 1    batch :  7000    loss : 0.00071    time :  5.717\n","epoch : 1    batch :  7040    loss : 0.00193    time :  5.720\n","epoch : 1    batch :  7080    loss : 0.00674    time :  5.707\n","epoch : 1    batch :  7120    loss : 0.00662    time :  5.741\n","epoch : 1    batch :  7160    loss : 0.00019    time :  5.724\n","epoch : 1    batch :  7200    loss : 0.00249    time :  5.770\n","epoch : 1    batch :  7240    loss : 0.00068    time :  5.742\n","epoch : 1    batch :  7280    loss : 0.00145    time :  5.791\n","epoch : 1    batch :  7320    loss : 0.00141    time :  5.756\n","epoch : 1    batch :  7360    loss : 0.00817    time :  5.763\n","epoch : 1    batch :  7400    loss : 0.00103    time :  5.746\n","epoch : 1    batch :  7440    loss : 0.00115    time :  5.761\n","epoch : 1    batch :  7480    loss : 0.00126    time :  5.751\n","epoch : 1    batch :  7520    loss : 0.00228    time :  5.765\n","epoch : 1    batch :  7560    loss : 0.00032    time :  5.738\n","epoch : 1    batch :  7600    loss : 0.00023    time :  5.784\n","epoch : 1    batch :  7640    loss : 0.00161    time :  5.755\n","epoch : 1    batch :  7680    loss : 0.00065    time :  5.770\n","epoch : 1    batch :  7720    loss : 0.00171    time :  5.743\n","epoch : 1    batch :  7760    loss : 0.00107    time :  5.747\n","epoch : 1    batch :  7800    loss : 0.00369    time :  5.767\n","epoch : 1    batch :  7840    loss : 0.00040    time :  5.730\n","epoch : 1    batch :  7880    loss : 0.00121    time :  5.814\n","epoch : 1    batch :  7920    loss : 0.00127    time :  5.741\n","epoch : 1    batch :  7960    loss : 0.00218    time :  5.736\n","epoch : 1    batch :  8000    loss : 0.00143    time :  6.021\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-12000.pt Data Loading Time : 156.521\n","epoch : 1    batch :  8040    loss : 0.00260    time : 167.482\n","epoch : 1    batch :  8080    loss : 0.00066    time :  5.748\n","epoch : 1    batch :  8120    loss : 0.00065    time :  5.776\n","epoch : 1    batch :  8160    loss : 0.00220    time :  5.774\n","epoch : 1    batch :  8200    loss : 0.00136    time :  5.761\n","epoch : 1    batch :  8240    loss : 0.00050    time :  5.785\n","epoch : 1    batch :  8280    loss : 0.00167    time :  5.784\n","epoch : 1    batch :  8320    loss : 0.00132    time :  5.766\n","epoch : 1    batch :  8360    loss : 0.00169    time :  5.770\n","epoch : 1    batch :  8400    loss : 0.00077    time :  5.782\n","epoch : 1    batch :  8440    loss : 0.00236    time :  5.770\n","epoch : 1    batch :  8480    loss : 0.00138    time :  5.769\n","epoch : 1    batch :  8520    loss : 0.00167    time :  5.739\n","epoch : 1    batch :  8560    loss : 0.00619    time :  5.774\n","epoch : 1    batch :  8600    loss : 0.00383    time :  5.752\n","epoch : 1    batch :  8640    loss : 0.00265    time :  5.784\n","epoch : 1    batch :  8680    loss : 0.00345    time :  5.763\n","epoch : 1    batch :  8720    loss : 0.00480    time :  5.766\n","epoch : 1    batch :  8760    loss : 0.00077    time :  5.778\n","epoch : 1    batch :  8800    loss : 0.00294    time :  5.777\n","epoch : 1    batch :  8840    loss : 0.00059    time :  5.778\n","epoch : 1    batch :  8880    loss : 0.00051    time :  5.770\n","epoch : 1    batch :  8920    loss : 0.00547    time :  5.761\n","epoch : 1    batch :  8960    loss : 0.00122    time :  5.778\n","epoch : 1    batch :  9000    loss : 0.00057    time :  5.744\n","epoch : 1    batch :  9040    loss : 0.00210    time :  5.753\n","epoch : 1    batch :  9080    loss : 0.00142    time :  5.705\n","epoch : 1    batch :  9120    loss : 0.00359    time :  5.724\n","epoch : 1    batch :  9160    loss : 0.00275    time :  5.709\n","epoch : 1    batch :  9200    loss : 0.00317    time :  5.747\n","epoch : 1    batch :  9240    loss : 0.00057    time :  5.719\n","epoch : 1    batch :  9280    loss : 0.00190    time :  5.752\n","epoch : 1    batch :  9320    loss : 0.00071    time :  5.715\n","epoch : 1    batch :  9360    loss : 0.00114    time :  5.767\n","epoch : 1    batch :  9400    loss : 0.00025    time :  5.685\n","epoch : 1    batch :  9440    loss : 0.00187    time :  5.742\n","epoch : 1    batch :  9480    loss : 0.00343    time :  5.993\n","epoch : 1    batch :  9520    loss : 0.00067    time :  5.745\n","epoch : 1    batch :  9560    loss : 0.00076    time :  5.751\n","epoch : 1    batch :  9600    loss : 0.00047    time :  5.765\n","epoch : 1    batch :  9640    loss : 0.00097    time :  5.772\n","epoch : 1    batch :  9680    loss : 0.00067    time :  5.775\n","epoch : 1    batch :  9720    loss : 0.00130    time :  5.774\n","epoch : 1    batch :  9760    loss : 0.00236    time :  5.808\n","epoch : 1    batch :  9800    loss : 0.00013    time :  5.767\n","epoch : 1    batch :  9840    loss : 0.00130    time :  5.786\n","epoch : 1    batch :  9880    loss : 0.00058    time :  5.760\n","epoch : 1    batch :  9920    loss : 0.00073    time :  5.812\n","epoch : 1    batch :  9960    loss : 0.00134    time :  5.803\n","epoch : 1    batch : 10000    loss : 0.00213    time :  5.784\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch : 10040    loss : 0.00148    time : 11.161\n","epoch : 1    batch : 10080    loss : 0.00015    time :  5.784\n","epoch : 1    batch : 10120    loss : 0.00152    time :  5.777\n","epoch : 1    batch : 10160    loss : 0.00203    time :  5.797\n","epoch : 1    batch : 10200    loss : 0.00360    time :  5.885\n","epoch : 1    batch : 10240    loss : 0.00075    time :  5.824\n","epoch : 1    batch : 10280    loss : 0.00021    time :  5.797\n","epoch : 1    batch : 10320    loss : 0.00239    time :  5.858\n","epoch : 1    batch : 10360    loss : 0.00054    time :  5.843\n","epoch : 1    batch : 10400    loss : 0.00262    time :  5.918\n","epoch : 1    batch : 10440    loss : 0.00033    time :  5.802\n","epoch : 1    batch : 10480    loss : 0.00078    time :  5.752\n","epoch : 1    batch : 10520    loss : 0.00026    time :  5.767\n","epoch : 1    batch : 10560    loss : 0.00074    time :  5.783\n","epoch : 1    batch : 10600    loss : 0.00116    time :  5.799\n","epoch : 1    batch : 10640    loss : 0.00387    time :  5.802\n","epoch : 1    batch : 10680    loss : 0.00129    time :  5.779\n","epoch : 1    batch : 10720    loss : 0.00045    time :  5.806\n","epoch : 1    batch : 10760    loss : 0.00069    time :  5.766\n","epoch : 1    batch : 10800    loss : 0.00294    time :  5.803\n","epoch : 1    batch : 10840    loss : 0.00213    time :  5.753\n","epoch : 1    batch : 10880    loss : 0.00008    time :  5.769\n","epoch : 1    batch : 10920    loss : 0.00020    time :  5.726\n","epoch : 1    batch : 10960    loss : 0.00111    time :  5.747\n","epoch : 1    batch : 11000    loss : 0.00099    time :  5.736\n","epoch : 1    batch : 11040    loss : 0.00100    time :  5.741\n","epoch : 1    batch : 11080    loss : 0.00030    time :  5.733\n","epoch : 1    batch : 11120    loss : 0.00367    time :  5.746\n","epoch : 1    batch : 11160    loss : 0.00029    time :  5.727\n","epoch : 1    batch : 11200    loss : 0.00050    time :  5.753\n","epoch : 1    batch : 11240    loss : 0.00020    time :  5.724\n","epoch : 1    batch : 11280    loss : 0.00196    time :  5.749\n","epoch : 1    batch : 11320    loss : 0.00040    time :  5.718\n","epoch : 1    batch : 11360    loss : 0.00392    time :  5.742\n","epoch : 1    batch : 11400    loss : 0.00547    time :  5.761\n","epoch : 1    batch : 11440    loss : 0.00081    time :  5.744\n","epoch : 1    batch : 11480    loss : 0.00063    time :  5.744\n","epoch : 1    batch : 11520    loss : 0.00099    time :  5.741\n","epoch : 1    batch : 11560    loss : 0.00222    time :  5.717\n","epoch : 1    batch : 11600    loss : 0.00072    time :  5.757\n","epoch : 1    batch : 11640    loss : 0.00161    time :  5.718\n","epoch : 1    batch : 11680    loss : 0.00513    time :  5.767\n","epoch : 1    batch : 11720    loss : 0.00076    time :  5.728\n","epoch : 1    batch : 11760    loss : 0.00387    time :  5.721\n","epoch : 1    batch : 11800    loss : 0.00147    time :  5.722\n","epoch : 1    batch : 11840    loss : 0.00140    time :  6.005\n","epoch : 1    batch : 11880    loss : 0.00027    time :  5.743\n","epoch : 1    batch : 11920    loss : 0.00275    time :  5.760\n","epoch : 1    batch : 11960    loss : 0.00077    time :  5.724\n","epoch : 1    batch : 12000    loss : 0.00156    time :  5.761\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-16000.pt Data Loading Time : 150.761\n","epoch : 1    batch : 12040    loss : 0.00080    time : 161.779\n","epoch : 1    batch : 12080    loss : 0.00075    time :  5.713\n","epoch : 1    batch : 12120    loss : 0.00054    time :  5.728\n","epoch : 1    batch : 12160    loss : 0.00050    time :  5.712\n","epoch : 1    batch : 12200    loss : 0.00257    time :  5.716\n","epoch : 1    batch : 12240    loss : 0.00047    time :  5.712\n","epoch : 1    batch : 12280    loss : 0.00237    time :  5.729\n","epoch : 1    batch : 12320    loss : 0.00041    time :  5.710\n","epoch : 1    batch : 12360    loss : 0.00027    time :  5.706\n","epoch : 1    batch : 12400    loss : 0.00077    time :  5.709\n","epoch : 1    batch : 12440    loss : 0.00037    time :  5.730\n","epoch : 1    batch : 12480    loss : 0.00036    time :  5.697\n","epoch : 1    batch : 12520    loss : 0.00209    time :  5.711\n","epoch : 1    batch : 12560    loss : 0.00062    time :  5.683\n","epoch : 1    batch : 12600    loss : 0.00028    time :  5.696\n","epoch : 1    batch : 12640    loss : 0.00052    time :  5.698\n","epoch : 1    batch : 12680    loss : 0.00133    time :  5.719\n","epoch : 1    batch : 12720    loss : 0.00131    time :  5.697\n","epoch : 1    batch : 12760    loss : 0.00105    time :  5.710\n","epoch : 1    batch : 12800    loss : 0.00071    time :  5.692\n","epoch : 1    batch : 12840    loss : 0.00029    time :  5.700\n","epoch : 1    batch : 12880    loss : 0.00127    time :  5.702\n","epoch : 1    batch : 12920    loss : 0.00015    time :  5.713\n","epoch : 1    batch : 12960    loss : 0.00095    time :  5.690\n","epoch : 1    batch : 13000    loss : 0.00065    time :  5.711\n","epoch : 1    batch : 13040    loss : 0.00376    time :  5.705\n","epoch : 1    batch : 13080    loss : 0.00009    time :  5.734\n","epoch : 1    batch : 13120    loss : 0.00097    time :  5.700\n","epoch : 1    batch : 13160    loss : 0.00548    time :  5.715\n","epoch : 1    batch : 13200    loss : 0.00097    time :  5.705\n","epoch : 1    batch : 13240    loss : 0.00102    time :  5.721\n","epoch : 1    batch : 13280    loss : 0.00112    time :  5.712\n","epoch : 1    batch : 13320    loss : 0.00070    time :  6.013\n","epoch : 1    batch : 13360    loss : 0.00346    time :  5.704\n","epoch : 1    batch : 13400    loss : 0.00103    time :  5.738\n","epoch : 1    batch : 13440    loss : 0.00222    time :  5.706\n","epoch : 1    batch : 13480    loss : 0.00006    time :  5.726\n","epoch : 1    batch : 13520    loss : 0.00078    time :  5.724\n","epoch : 1    batch : 13560    loss : 0.00354    time :  5.708\n","epoch : 1    batch : 13600    loss : 0.00204    time :  5.697\n","epoch : 1    batch : 13640    loss : 0.00308    time :  5.698\n","epoch : 1    batch : 13680    loss : 0.00147    time :  5.738\n","epoch : 1    batch : 13720    loss : 0.00358    time :  5.706\n","epoch : 1    batch : 13760    loss : 0.00197    time :  5.712\n","epoch : 1    batch : 13800    loss : 0.00218    time :  5.702\n","epoch : 1    batch : 13840    loss : 0.00256    time :  5.720\n","epoch : 1    batch : 13880    loss : 0.00258    time :  5.697\n","epoch : 1    batch : 13920    loss : 0.00048    time :  5.728\n","epoch : 1    batch : 13960    loss : 0.00068    time :  5.703\n","epoch : 1    batch : 14000    loss : 0.00026    time :  5.710\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch : 14040    loss : 0.00123    time : 11.604\n","epoch : 1    batch : 14080    loss : 0.00074    time :  5.711\n","epoch : 1    batch : 14120    loss : 0.00047    time :  5.840\n","epoch : 1    batch : 14160    loss : 0.00241    time :  5.804\n","epoch : 1    batch : 14200    loss : 0.00105    time :  5.766\n","epoch : 1    batch : 14240    loss : 0.00077    time :  5.723\n","epoch : 1    batch : 14280    loss : 0.00127    time :  5.731\n","epoch : 1    batch : 14320    loss : 0.00121    time :  5.738\n","epoch : 1    batch : 14360    loss : 0.00024    time :  5.765\n","epoch : 1    batch : 14400    loss : 0.00116    time :  5.740\n","epoch : 1    batch : 14440    loss : 0.00073    time :  5.760\n","epoch : 1    batch : 14480    loss : 0.00051    time :  5.703\n","epoch : 1    batch : 14520    loss : 0.00048    time :  5.728\n","epoch : 1    batch : 14560    loss : 0.00067    time :  5.691\n","epoch : 1    batch : 14600    loss : 0.00125    time :  5.710\n","epoch : 1    batch : 14640    loss : 0.00322    time :  5.694\n","epoch : 1    batch : 14680    loss : 0.00014    time :  5.709\n","epoch : 1    batch : 14720    loss : 0.00058    time :  5.693\n","epoch : 1    batch : 14760    loss : 0.00018    time :  5.730\n","epoch : 1    batch : 14800    loss : 0.00036    time :  5.693\n","epoch : 1    batch : 14840    loss : 0.00041    time :  5.738\n","epoch : 1    batch : 14880    loss : 0.00040    time :  5.697\n","epoch : 1    batch : 14920    loss : 0.00035    time :  5.719\n","epoch : 1    batch : 14960    loss : 0.00356    time :  5.705\n","epoch : 1    batch : 15000    loss : 0.00257    time :  5.711\n","epoch : 1    batch : 15040    loss : 0.00029    time :  5.706\n","epoch : 1    batch : 15080    loss : 0.00184    time :  5.708\n","epoch : 1    batch : 15120    loss : 0.00061    time :  5.695\n","epoch : 1    batch : 15160    loss : 0.00029    time :  5.692\n","epoch : 1    batch : 15200    loss : 0.00138    time :  5.715\n","epoch : 1    batch : 15240    loss : 0.00912    time :  5.929\n","epoch : 1    batch : 15280    loss : 0.00282    time :  5.707\n","epoch : 1    batch : 15320    loss : 0.00037    time :  5.703\n","epoch : 1    batch : 15360    loss : 0.00119    time :  5.711\n","epoch : 1    batch : 15400    loss : 0.00279    time :  5.698\n","epoch : 1    batch : 15440    loss : 0.00202    time :  5.722\n","epoch : 1    batch : 15480    loss : 0.00271    time :  5.682\n","epoch : 1    batch : 15520    loss : 0.00137    time :  5.699\n","epoch : 1    batch : 15560    loss : 0.00195    time :  5.681\n","epoch : 1    batch : 15600    loss : 0.00309    time :  5.714\n","epoch : 1    batch : 15640    loss : 0.00449    time :  5.688\n","epoch : 1    batch : 15680    loss : 0.00710    time :  5.715\n","epoch : 1    batch : 15720    loss : 0.00309    time :  5.696\n","epoch : 1    batch : 15760    loss : 0.00151    time :  5.700\n","epoch : 1    batch : 15800    loss : 0.00072    time :  5.688\n","epoch : 1    batch : 15840    loss : 0.00251    time :  5.696\n","epoch : 1    batch : 15880    loss : 0.00104    time :  5.685\n","epoch : 1    batch : 15920    loss : 0.00247    time :  5.710\n","epoch : 1    batch : 15960    loss : 0.00029    time :  5.685\n","epoch : 1    batch : 16000    loss : 0.00125    time :  5.726\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-20000.pt Data Loading Time : 131.096\n","epoch : 1    batch : 16040    loss : 0.00179    time : 142.027\n","epoch : 1    batch : 16080    loss : 0.00044    time :  5.700\n","epoch : 1    batch : 16120    loss : 0.00062    time :  5.710\n","epoch : 1    batch : 16160    loss : 0.00103    time :  5.706\n","epoch : 1    batch : 16200    loss : 0.00073    time :  5.716\n","epoch : 1    batch : 16240    loss : 0.00157    time :  5.728\n","epoch : 1    batch : 16280    loss : 0.00117    time :  5.706\n","epoch : 1    batch : 16320    loss : 0.00057    time :  5.707\n","epoch : 1    batch : 16360    loss : 0.00013    time :  5.694\n","epoch : 1    batch : 16400    loss : 0.00221    time :  5.723\n","epoch : 1    batch : 16440    loss : 0.00203    time :  5.957\n","epoch : 1    batch : 16480    loss : 0.00207    time :  5.746\n","epoch : 1    batch : 16520    loss : 0.00065    time :  5.725\n","epoch : 1    batch : 16560    loss : 0.00212    time :  5.723\n","epoch : 1    batch : 16600    loss : 0.00120    time :  5.704\n","epoch : 1    batch : 16640    loss : 0.00187    time :  5.741\n","epoch : 1    batch : 16680    loss : 0.00053    time :  5.729\n","epoch : 1    batch : 16720    loss : 0.00019    time :  5.726\n","epoch : 1    batch : 16760    loss : 0.00120    time :  5.722\n","epoch : 1    batch : 16800    loss : 0.00082    time :  5.723\n","epoch : 1    batch : 16840    loss : 0.00012    time :  5.725\n","epoch : 1    batch : 16880    loss : 0.00064    time :  5.740\n","epoch : 1    batch : 16920    loss : 0.00114    time :  5.733\n","epoch : 1    batch : 16960    loss : 0.00078    time :  5.740\n","epoch : 1    batch : 17000    loss : 0.00173    time :  5.717\n","epoch : 1    batch : 17040    loss : 0.00020    time :  5.736\n","epoch : 1    batch : 17080    loss : 0.00053    time :  5.740\n","epoch : 1    batch : 17120    loss : 0.00044    time :  5.742\n","epoch : 1    batch : 17160    loss : 0.00102    time :  5.741\n","epoch : 1    batch : 17200    loss : 0.00153    time :  5.758\n","epoch : 1    batch : 17240    loss : 0.00108    time :  5.724\n","epoch : 1    batch : 17280    loss : 0.00076    time :  5.751\n","epoch : 1    batch : 17320    loss : 0.00161    time :  5.703\n","epoch : 1    batch : 17360    loss : 0.00015    time :  5.741\n","epoch : 1    batch : 17400    loss : 0.00153    time :  5.708\n","epoch : 1    batch : 17440    loss : 0.00026    time :  5.745\n","epoch : 1    batch : 17480    loss : 0.00045    time :  5.715\n","epoch : 1    batch : 17520    loss : 0.00295    time :  5.733\n","epoch : 1    batch : 17560    loss : 0.00093    time :  5.730\n","epoch : 1    batch : 17600    loss : 0.00018    time :  5.737\n","epoch : 1    batch : 17640    loss : 0.00024    time :  5.706\n","epoch : 1    batch : 17680    loss : 0.00162    time :  5.742\n","epoch : 1    batch : 17720    loss : 0.00154    time :  5.733\n","epoch : 1    batch : 17760    loss : 0.00081    time :  5.718\n","epoch : 1    batch : 17800    loss : 0.00139    time :  5.719\n","epoch : 1    batch : 17840    loss : 0.00080    time :  5.715\n","epoch : 1    batch : 17880    loss : 0.00534    time :  5.740\n","epoch : 1    batch : 17920    loss : 0.00174    time :  5.713\n","epoch : 1    batch : 17960    loss : 0.00232    time :  5.719\n","epoch : 1    batch : 18000    loss : 0.00050    time :  5.718\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch : 18040    loss : 0.00233    time : 11.883\n","epoch : 1    batch : 18080    loss : 0.00075    time :  5.780\n","epoch : 1    batch : 18120    loss : 0.00200    time :  5.836\n","epoch : 1    batch : 18160    loss : 0.00071    time :  5.902\n","epoch : 1    batch : 18200    loss : 0.00128    time :  5.914\n","epoch : 1    batch : 18240    loss : 0.00214    time :  5.832\n","epoch : 1    batch : 18280    loss : 0.00103    time :  5.745\n","epoch : 1    batch : 18320    loss : 0.00492    time :  5.790\n","epoch : 1    batch : 18360    loss : 0.00180    time :  5.727\n","epoch : 1    batch : 18400    loss : 0.00066    time :  5.782\n","epoch : 1    batch : 18440    loss : 0.00638    time :  5.736\n","epoch : 1    batch : 18480    loss : 0.00279    time :  5.736\n","epoch : 1    batch : 18520    loss : 0.00213    time :  5.719\n","epoch : 1    batch : 18560    loss : 0.00206    time :  5.735\n","epoch : 1    batch : 18600    loss : 0.00041    time :  5.718\n","epoch : 1    batch : 18640    loss : 0.00053    time :  5.732\n","epoch : 1    batch : 18680    loss : 0.00140    time :  5.711\n","epoch : 1    batch : 18720    loss : 0.00302    time :  5.738\n","epoch : 1    batch : 18760    loss : 0.00050    time :  5.708\n","epoch : 1    batch : 18800    loss : 0.00177    time :  5.716\n","epoch : 1    batch : 18840    loss : 0.00327    time :  5.714\n","epoch : 1    batch : 18880    loss : 0.00059    time :  5.732\n","epoch : 1    batch : 18920    loss : 0.00013    time :  5.704\n","epoch : 1    batch : 18960    loss : 0.00129    time :  5.728\n","epoch : 1    batch : 19000    loss : 0.00072    time :  5.723\n","epoch : 1    batch : 19040    loss : 0.00044    time :  5.722\n","epoch : 1    batch : 19080    loss : 0.00111    time :  5.708\n","epoch : 1    batch : 19120    loss : 0.00140    time :  5.727\n","epoch : 1    batch : 19160    loss : 0.00043    time :  5.709\n","epoch : 1    batch : 19200    loss : 0.00200    time :  6.035\n","epoch : 1    batch : 19240    loss : 0.00269    time :  5.729\n","epoch : 1    batch : 19280    loss : 0.00013    time :  5.741\n","epoch : 1    batch : 19320    loss : 0.00052    time :  5.746\n","epoch : 1    batch : 19360    loss : 0.00032    time :  5.736\n","epoch : 1    batch : 19400    loss : 0.00093    time :  5.715\n","epoch : 1    batch : 19440    loss : 0.00065    time :  5.719\n","epoch : 1    batch : 19480    loss : 0.00040    time :  5.725\n","epoch : 1    batch : 19520    loss : 0.00012    time :  5.723\n","epoch : 1    batch : 19560    loss : 0.00048    time :  5.729\n","epoch : 1    batch : 19600    loss : 0.00191    time :  5.701\n","epoch : 1    batch : 19640    loss : 0.00083    time :  5.714\n","epoch : 1    batch : 19680    loss : 0.00048    time :  5.728\n","epoch : 1    batch : 19720    loss : 0.00081    time :  5.722\n","epoch : 1    batch : 19760    loss : 0.00016    time :  5.699\n","epoch : 1    batch : 19800    loss : 0.00055    time :  5.731\n","epoch : 1    batch : 19840    loss : 0.00079    time :  5.721\n","epoch : 1    batch : 19880    loss : 0.00149    time :  5.733\n","epoch : 1    batch : 19920    loss : 0.00100    time :  5.697\n","epoch : 1    batch : 19960    loss : 0.00199    time :  5.710\n","epoch : 1    batch : 20000    loss : 0.00055    time :  5.703\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-24000.pt Data Loading Time : 142.301\n","epoch : 1    batch : 20040    loss : 0.00025    time : 153.382\n","epoch : 1    batch : 20080    loss : 0.00008    time :  5.725\n","epoch : 1    batch : 20120    loss : 0.00023    time :  5.737\n","epoch : 1    batch : 20160    loss : 0.00061    time :  5.729\n","epoch : 1    batch : 20200    loss : 0.00049    time :  6.027\n","epoch : 1    batch : 20240    loss : 0.00010    time :  5.717\n","epoch : 1    batch : 20280    loss : 0.00037    time :  5.735\n","epoch : 1    batch : 20320    loss : 0.00079    time :  5.718\n","epoch : 1    batch : 20360    loss : 0.00011    time :  5.729\n","epoch : 1    batch : 20400    loss : 0.00007    time :  5.716\n","epoch : 1    batch : 20440    loss : 0.00046    time :  5.753\n","epoch : 1    batch : 20480    loss : 0.00020    time :  5.720\n","epoch : 1    batch : 20520    loss : 0.00061    time :  5.720\n","epoch : 1    batch : 20560    loss : 0.00028    time :  5.708\n","epoch : 1    batch : 20600    loss : 0.00124    time :  5.726\n","epoch : 1    batch : 20640    loss : 0.00028    time :  5.727\n","epoch : 1    batch : 20680    loss : 0.00017    time :  5.724\n","epoch : 1    batch : 20720    loss : 0.00309    time :  5.702\n","epoch : 1    batch : 20760    loss : 0.00190    time :  5.719\n","epoch : 1    batch : 20800    loss : 0.00038    time :  5.709\n","epoch : 1    batch : 20840    loss : 0.00022    time :  5.723\n","epoch : 1    batch : 20880    loss : 0.00055    time :  5.705\n","epoch : 1    batch : 20920    loss : 0.00030    time :  5.726\n","epoch : 1    batch : 20960    loss : 0.00054    time :  5.733\n","epoch : 1    batch : 21000    loss : 0.00026    time :  5.720\n","epoch : 1    batch : 21040    loss : 0.00029    time :  5.710\n","epoch : 1    batch : 21080    loss : 0.00023    time :  5.725\n","epoch : 1    batch : 21120    loss : 0.00096    time :  5.712\n","epoch : 1    batch : 21160    loss : 0.00029    time :  5.743\n","epoch : 1    batch : 21200    loss : 0.00037    time :  5.746\n","epoch : 1    batch : 21240    loss : 0.00038    time :  5.732\n","epoch : 1    batch : 21280    loss : 0.00130    time :  5.724\n","epoch : 1    batch : 21320    loss : 0.00007    time :  5.746\n","epoch : 1    batch : 21360    loss : 0.00041    time :  5.714\n","epoch : 1    batch : 21400    loss : 0.00074    time :  5.760\n","epoch : 1    batch : 21440    loss : 0.00066    time :  5.723\n","epoch : 1    batch : 21480    loss : 0.00028    time :  5.739\n","epoch : 1    batch : 21520    loss : 0.00355    time :  5.727\n","epoch : 1    batch : 21560    loss : 0.00105    time :  5.720\n","epoch : 1    batch : 21600    loss : 0.00144    time :  5.722\n","epoch : 1    batch : 21640    loss : 0.00006    time :  5.731\n","epoch : 1    batch : 21680    loss : 0.00034    time :  5.726\n","epoch : 1    batch : 21720    loss : 0.00104    time :  5.725\n","epoch : 1    batch : 21760    loss : 0.00034    time :  5.739\n","epoch : 1    batch : 21800    loss : 0.00213    time :  5.736\n","epoch : 1    batch : 21840    loss : 0.00166    time :  5.741\n","epoch : 1    batch : 21880    loss : 0.00065    time :  5.996\n","epoch : 1    batch : 21920    loss : 0.00593    time :  5.730\n","epoch : 1    batch : 21960    loss : 0.00153    time :  5.706\n","epoch : 1    batch : 22000    loss : 0.00913    time :  5.728\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch : 22040    loss : 0.00891    time : 11.072\n","epoch : 1    batch : 22080    loss : 0.00740    time :  5.752\n","epoch : 1    batch : 22120    loss : 0.00047    time :  5.795\n","epoch : 1    batch : 22160    loss : 0.00331    time :  5.758\n","epoch : 1    batch : 22200    loss : 0.00395    time :  5.856\n","epoch : 1    batch : 22240    loss : 0.00085    time :  5.759\n","epoch : 1    batch : 22280    loss : 0.00077    time :  5.757\n","epoch : 1    batch : 22320    loss : 0.00187    time :  5.736\n","epoch : 1    batch : 22360    loss : 0.00023    time :  5.815\n","epoch : 1    batch : 22400    loss : 0.00043    time :  5.730\n","epoch : 1    batch : 22440    loss : 0.00039    time :  5.765\n","epoch : 1    batch : 22480    loss : 0.00053    time :  5.709\n","epoch : 1    batch : 22520    loss : 0.00090    time :  5.726\n","epoch : 1    batch : 22560    loss : 0.00051    time :  5.710\n","epoch : 1    batch : 22600    loss : 0.00100    time :  5.732\n","epoch : 1    batch : 22640    loss : 0.00052    time :  5.726\n","epoch : 1    batch : 22680    loss : 0.00093    time :  5.729\n","epoch : 1    batch : 22720    loss : 0.00139    time :  5.717\n","epoch : 1    batch : 22760    loss : 0.00039    time :  5.725\n","epoch : 1    batch : 22800    loss : 0.00005    time :  5.721\n","epoch : 1    batch : 22840    loss : 0.00068    time :  5.737\n","epoch : 1    batch : 22880    loss : 0.00047    time :  5.714\n","epoch : 1    batch : 22920    loss : 0.00027    time :  5.724\n","epoch : 1    batch : 22960    loss : 0.00130    time :  5.728\n","epoch : 1    batch : 23000    loss : 0.00276    time :  5.734\n","epoch : 1    batch : 23040    loss : 0.00055    time :  5.712\n","epoch : 1    batch : 23080    loss : 0.00113    time :  5.717\n","epoch : 1    batch : 23120    loss : 0.00006    time :  5.713\n","epoch : 1    batch : 23160    loss : 0.00289    time :  5.707\n","epoch : 1    batch : 23200    loss : 0.00033    time :  5.725\n","epoch : 1    batch : 23240    loss : 0.00041    time :  5.714\n","epoch : 1    batch : 23280    loss : 0.00131    time :  5.750\n","epoch : 1    batch : 23320    loss : 0.00295    time :  5.714\n","epoch : 1    batch : 23360    loss : 0.00252    time :  5.717\n","epoch : 1    batch : 23400    loss : 0.00071    time :  5.707\n","epoch : 1    batch : 23440    loss : 0.00046    time :  5.725\n","epoch : 1    batch : 23480    loss : 0.00088    time :  5.708\n","epoch : 1    batch : 23520    loss : 0.00116    time :  5.706\n","epoch : 1    batch : 23560    loss : 0.00105    time :  5.685\n","epoch : 1    batch : 23600    loss : 0.00039    time :  5.708\n","epoch : 1    batch : 23640    loss : 0.00007    time :  5.703\n","epoch : 1    batch : 23680    loss : 0.00496    time :  5.723\n","epoch : 1    batch : 23720    loss : 0.00048    time :  5.732\n","epoch : 1    batch : 23760    loss : 0.00118    time :  5.746\n","epoch : 1    batch : 23800    loss : 0.00016    time :  5.720\n","epoch : 1    batch : 23840    loss : 0.00246    time :  5.755\n","epoch : 1    batch : 23880    loss : 0.00217    time :  5.702\n","epoch : 1    batch : 23920    loss : 0.00072    time :  5.729\n","epoch : 1    batch : 23960    loss : 0.00204    time :  5.704\n","epoch : 1    batch : 24000    loss : 0.00191    time :  5.723\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-28000.pt Data Loading Time : 145.500\n","epoch : 1    batch : 24040    loss : 0.00076    time : 159.526\n","epoch : 1    batch : 24080    loss : 0.00048    time :  5.708\n","epoch : 1    batch : 24120    loss : 0.00326    time :  5.718\n","epoch : 1    batch : 24160    loss : 0.00132    time :  5.709\n","epoch : 1    batch : 24200    loss : 0.00025    time :  5.720\n","epoch : 1    batch : 24240    loss : 0.00031    time :  5.704\n","epoch : 1    batch : 24280    loss : 0.00026    time :  5.707\n","epoch : 1    batch : 24320    loss : 0.00141    time :  5.714\n","epoch : 1    batch : 24360    loss : 0.00067    time :  5.726\n","epoch : 1    batch : 24400    loss : 0.00111    time :  5.721\n","epoch : 1    batch : 24440    loss : 0.00071    time :  5.728\n","epoch : 1    batch : 24480    loss : 0.00029    time :  5.710\n","epoch : 1    batch : 24520    loss : 0.00058    time :  5.712\n","epoch : 1    batch : 24560    loss : 0.00054    time :  5.706\n","epoch : 1    batch : 24600    loss : 0.00185    time :  5.725\n","epoch : 1    batch : 24640    loss : 0.00147    time :  5.700\n","epoch : 1    batch : 24680    loss : 0.00020    time :  5.714\n","epoch : 1    batch : 24720    loss : 0.00044    time :  5.696\n","epoch : 1    batch : 24760    loss : 0.00034    time :  5.716\n","epoch : 1    batch : 24800    loss : 0.00098    time :  5.701\n","epoch : 1    batch : 24840    loss : 0.00038    time :  5.715\n","epoch : 1    batch : 24880    loss : 0.00043    time :  5.697\n","epoch : 1    batch : 24920    loss : 0.00232    time :  5.710\n","epoch : 1    batch : 24960    loss : 0.00350    time :  5.698\n","epoch : 1    batch : 25000    loss : 0.00029    time :  5.721\n","epoch : 1    batch : 25040    loss : 0.00098    time :  5.712\n","epoch : 1    batch : 25080    loss : 0.00079    time :  5.723\n","epoch : 1    batch : 25120    loss : 0.00030    time :  5.713\n","epoch : 1    batch : 25160    loss : 0.00055    time :  5.731\n","epoch : 1    batch : 25200    loss : 0.00184    time :  5.723\n","epoch : 1    batch : 25240    loss : 0.00324    time :  5.730\n","epoch : 1    batch : 25280    loss : 0.00049    time :  5.717\n","epoch : 1    batch : 25320    loss : 0.00150    time :  5.722\n","epoch : 1    batch : 25360    loss : 0.00081    time :  5.947\n","epoch : 1    batch : 25400    loss : 0.00190    time :  5.728\n","epoch : 1    batch : 25440    loss : 0.00057    time :  5.725\n","epoch : 1    batch : 25480    loss : 0.00164    time :  5.735\n","epoch : 1    batch : 25520    loss : 0.00064    time :  5.708\n","epoch : 1    batch : 25560    loss : 0.00038    time :  5.732\n","epoch : 1    batch : 25600    loss : 0.00170    time :  5.721\n","epoch : 1    batch : 25640    loss : 0.00010    time :  5.730\n","epoch : 1    batch : 25680    loss : 0.00054    time :  5.745\n","epoch : 1    batch : 25720    loss : 0.00057    time :  5.702\n","epoch : 1    batch : 25760    loss : 0.00033    time :  5.720\n","epoch : 1    batch : 25800    loss : 0.00036    time :  5.737\n","epoch : 1    batch : 25840    loss : 0.00013    time :  5.755\n","epoch : 1    batch : 25880    loss : 0.00010    time :  5.723\n","epoch : 1    batch : 25920    loss : 0.00237    time :  5.737\n","epoch : 1    batch : 25960    loss : 0.00069    time :  5.725\n","epoch : 1    batch : 26000    loss : 0.00024    time :  5.744\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch : 26040    loss : 0.00024    time : 12.500\n","epoch : 1    batch : 26080    loss : 0.00422    time :  5.798\n","epoch : 1    batch : 26120    loss : 0.02891    time :  5.855\n","epoch : 1    batch : 26160    loss : 0.00351    time :  5.884\n","epoch : 1    batch : 26200    loss : 0.00063    time :  5.852\n","epoch : 1    batch : 26240    loss : 0.00103    time :  5.754\n","epoch : 1    batch : 26280    loss : 0.00101    time :  5.777\n","epoch : 1    batch : 26320    loss : 0.00100    time :  5.775\n","epoch : 1    batch : 26360    loss : 0.00133    time :  5.746\n","epoch : 1    batch : 26400    loss : 0.00096    time :  5.732\n","epoch : 1    batch : 26440    loss : 0.00073    time :  5.779\n","epoch : 1    batch : 26480    loss : 0.00054    time :  5.792\n","epoch : 1    batch : 26520    loss : 0.00166    time :  5.763\n","epoch : 1    batch : 26560    loss : 0.00332    time :  5.732\n","epoch : 1    batch : 26600    loss : 0.00030    time :  5.736\n","epoch : 1    batch : 26640    loss : 0.00130    time :  5.712\n","epoch : 1    batch : 26680    loss : 0.00448    time :  5.728\n","epoch : 1    batch : 26720    loss : 0.00701    time :  5.702\n","epoch : 1    batch : 26760    loss : 0.00031    time :  5.739\n","epoch : 1    batch : 26800    loss : 0.00047    time :  5.718\n","epoch : 1    batch : 26840    loss : 0.00172    time :  5.726\n","epoch : 1    batch : 26880    loss : 0.00086    time :  5.712\n","epoch : 1    batch : 26920    loss : 0.00138    time :  5.737\n","epoch : 1    batch : 26960    loss : 0.00159    time :  5.719\n","epoch : 1    batch : 27000    loss : 0.00168    time :  5.733\n","epoch : 1    batch : 27040    loss : 0.00080    time :  5.730\n","epoch : 1    batch : 27080    loss : 0.00016    time :  5.738\n","epoch : 1    batch : 27120    loss : 0.00072    time :  5.715\n","epoch : 1    batch : 27160    loss : 0.00053    time :  5.733\n","epoch : 1    batch : 27200    loss : 0.00008    time :  5.726\n","epoch : 1    batch : 27240    loss : 0.00054    time :  5.736\n","epoch : 1    batch : 27280    loss : 0.00064    time :  5.709\n","epoch : 1    batch : 27320    loss : 0.00034    time :  5.732\n","epoch : 1    batch : 27360    loss : 0.00136    time :  5.730\n","epoch : 1    batch : 27400    loss : 0.00125    time :  5.722\n","epoch : 1    batch : 27440    loss : 0.00430    time :  5.728\n","epoch : 1    batch : 27480    loss : 0.00173    time :  5.723\n","epoch : 1    batch : 27520    loss : 0.00479    time :  5.721\n","epoch : 1    batch : 27560    loss : 0.00158    time :  5.734\n","epoch : 1    batch : 27600    loss : 0.00285    time :  5.741\n","epoch : 1    batch : 27640    loss : 0.00719    time :  5.732\n","epoch : 1    batch : 27680    loss : 0.00080    time :  5.735\n","epoch : 1    batch : 27720    loss : 0.00191    time :  5.709\n","epoch : 1    batch : 27760    loss : 0.00612    time :  5.720\n","epoch : 1    batch : 27800    loss : 0.00018    time :  5.710\n","epoch : 1    batch : 27840    loss : 0.00166    time :  5.741\n","epoch : 1    batch : 27880    loss : 0.00009    time :  5.709\n","epoch : 1    batch : 27920    loss : 0.00184    time :  5.721\n","epoch : 1    batch : 27960    loss : 0.00025    time :  5.705\n","epoch : 1    batch : 28000    loss : 0.00013    time :  5.725\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-32000.pt Data Loading Time : 172.781\n","epoch : 1    batch : 28040    loss : 0.00223    time : 183.791\n","epoch : 1    batch : 28080    loss : 0.00135    time :  5.732\n","epoch : 1    batch : 28120    loss : 0.00263    time :  5.721\n","epoch : 1    batch : 28160    loss : 0.00505    time :  5.716\n","epoch : 1    batch : 28200    loss : 0.00664    time :  5.708\n","epoch : 1    batch : 28240    loss : 0.00113    time :  5.733\n","epoch : 1    batch : 28280    loss : 0.00130    time :  5.715\n","epoch : 1    batch : 28320    loss : 0.00139    time :  5.721\n","epoch : 1    batch : 28360    loss : 0.00097    time :  5.714\n","epoch : 1    batch : 28400    loss : 0.00025    time :  5.727\n","epoch : 1    batch : 28440    loss : 0.00718    time :  5.733\n","epoch : 1    batch : 28480    loss : 0.00057    time :  5.727\n","epoch : 1    batch : 28520    loss : 0.00235    time :  5.734\n","epoch : 1    batch : 28560    loss : 0.00211    time :  5.730\n","epoch : 1    batch : 28600    loss : 0.00123    time :  5.721\n","epoch : 1    batch : 28640    loss : 0.00085    time :  5.712\n","epoch : 1    batch : 28680    loss : 0.00018    time :  5.710\n","epoch : 1    batch : 28720    loss : 0.00052    time :  5.723\n","epoch : 1    batch : 28760    loss : 0.00287    time :  5.726\n","epoch : 1    batch : 28800    loss : 0.00168    time :  5.719\n","epoch : 1    batch : 28840    loss : 0.00068    time :  5.726\n","epoch : 1    batch : 28880    loss : 0.00009    time :  5.722\n","epoch : 1    batch : 28920    loss : 0.00253    time :  5.728\n","epoch : 1    batch : 28960    loss : 0.00424    time :  5.707\n","epoch : 1    batch : 29000    loss : 0.00030    time :  5.950\n","epoch : 1    batch : 29040    loss : 0.00164    time :  5.703\n","epoch : 1    batch : 29080    loss : 0.00074    time :  5.719\n","epoch : 1    batch : 29120    loss : 0.00179    time :  5.705\n","epoch : 1    batch : 29160    loss : 0.00260    time :  5.734\n","epoch : 1    batch : 29200    loss : 0.00121    time :  5.711\n","epoch : 1    batch : 29240    loss : 0.00053    time :  5.735\n","epoch : 1    batch : 29280    loss : 0.00103    time :  5.722\n","epoch : 1    batch : 29320    loss : 0.00038    time :  5.725\n","epoch : 1    batch : 29360    loss : 0.00106    time :  5.715\n","epoch : 1    batch : 29400    loss : 0.00098    time :  5.750\n","epoch : 1    batch : 29440    loss : 0.00096    time :  5.725\n","epoch : 1    batch : 29480    loss : 0.00274    time :  5.729\n","epoch : 1    batch : 29520    loss : 0.00034    time :  5.718\n","epoch : 1    batch : 29560    loss : 0.00096    time :  5.732\n","epoch : 1    batch : 29600    loss : 0.00041    time :  5.721\n","epoch : 1    batch : 29640    loss : 0.00022    time :  5.742\n","epoch : 1    batch : 29680    loss : 0.00182    time :  5.749\n","epoch : 1    batch : 29720    loss : 0.00034    time :  5.739\n","epoch : 1    batch : 29760    loss : 0.00030    time :  5.740\n","epoch : 1    batch : 29800    loss : 0.00052    time :  5.739\n","epoch : 1    batch : 29840    loss : 0.00073    time :  5.719\n","epoch : 1    batch : 29880    loss : 0.00049    time :  5.744\n","epoch : 1    batch : 29920    loss : 0.00099    time :  5.734\n","epoch : 1    batch : 29960    loss : 0.00073    time :  5.742\n","epoch : 1    batch : 30000    loss : 0.00025    time :  5.740\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch : 30040    loss : 0.00013    time : 11.251\n","epoch : 1    batch : 30080    loss : 0.00008    time :  5.845\n","epoch : 1    batch : 30120    loss : 0.00277    time :  5.883\n","epoch : 1    batch : 30160    loss : 0.00130    time :  5.831\n","epoch : 1    batch : 30200    loss : 0.00021    time :  5.799\n","epoch : 1    batch : 30240    loss : 0.00279    time :  5.786\n","epoch : 1    batch : 30280    loss : 0.00027    time :  5.787\n","epoch : 1    batch : 30320    loss : 0.00238    time :  5.818\n","epoch : 1    batch : 30360    loss : 0.00853    time :  5.831\n","epoch : 1    batch : 30400    loss : 0.00327    time :  5.805\n","epoch : 1    batch : 30440    loss : 0.00245    time :  5.773\n","epoch : 1    batch : 30480    loss : 0.00040    time :  5.765\n","epoch : 1    batch : 30520    loss : 0.00118    time :  5.747\n","epoch : 1    batch : 30560    loss : 0.01431    time :  5.738\n","epoch : 1    batch : 30600    loss : 0.00558    time :  5.750\n","epoch : 1    batch : 30640    loss : 0.00201    time :  5.759\n","epoch : 1    batch : 30680    loss : 0.00805    time :  5.738\n","epoch : 1    batch : 30720    loss : 0.00487    time :  5.741\n","epoch : 1    batch : 30760    loss : 0.00299    time :  5.755\n","epoch : 1    batch : 30800    loss : 0.00050    time :  5.717\n","epoch : 1    batch : 30840    loss : 0.00185    time :  5.774\n","epoch : 1    batch : 30880    loss : 0.00290    time :  5.751\n","epoch : 1    batch : 30920    loss : 0.00091    time :  5.760\n","epoch : 1    batch : 30960    loss : 0.00079    time :  5.718\n","epoch : 1    batch : 31000    loss : 0.02003    time :  5.779\n","epoch : 1    batch : 31040    loss : 0.01170    time :  5.799\n","epoch : 1    batch : 31080    loss : 0.01777    time :  5.826\n","epoch : 1    batch : 31120    loss : 0.00238    time :  5.782\n","epoch : 1    batch : 31160    loss : 0.00584    time :  5.789\n","epoch : 1    batch : 31200    loss : 0.01332    time :  5.754\n","epoch : 1    batch : 31240    loss : 0.02004    time :  5.726\n","epoch : 1    batch : 31280    loss : 0.00419    time :  5.738\n","epoch : 1    batch : 31320    loss : 0.00580    time :  5.768\n","epoch : 1    batch : 31360    loss : 0.00060    time :  5.725\n","epoch : 1    batch : 31400    loss : 0.00173    time :  5.743\n","epoch : 1    batch : 31440    loss : 0.00066    time :  5.737\n","epoch : 1    batch : 31480    loss : 0.00503    time :  5.751\n","epoch : 1    batch : 31520    loss : 0.00034    time :  5.727\n","epoch : 1    batch : 31560    loss : 0.00078    time :  5.727\n","epoch : 1    batch : 31600    loss : 0.00050    time :  5.727\n","epoch : 1    batch : 31640    loss : 0.00204    time :  5.744\n","epoch : 1    batch : 31680    loss : 0.00028    time :  6.008\n","epoch : 1    batch : 31720    loss : 0.00250    time :  5.751\n","epoch : 1    batch : 31760    loss : 0.00299    time :  5.727\n","epoch : 1    batch : 31800    loss : 0.00148    time :  5.730\n","epoch : 1    batch : 31840    loss : 0.00119    time :  5.729\n","epoch : 1    batch : 31880    loss : 0.00188    time :  5.736\n","epoch : 1    batch : 31920    loss : 0.00097    time :  5.700\n","epoch : 1    batch : 31960    loss : 0.00115    time :  5.717\n","epoch : 1    batch : 32000    loss : 0.00010    time :  5.711\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-36000.pt Data Loading Time : 91.309\n","epoch : 1    batch : 32040    loss : 0.00322    time : 102.281\n","epoch : 1    batch : 32080    loss : 0.00217    time :  5.729\n","epoch : 1    batch : 32120    loss : 0.00131    time :  5.741\n","epoch : 1    batch : 32160    loss : 0.00273    time :  5.720\n","epoch : 1    batch : 32200    loss : 0.00013    time :  5.712\n","epoch : 1    batch : 32240    loss : 0.00083    time :  5.727\n","epoch : 1    batch : 32280    loss : 0.00082    time :  5.734\n","epoch : 1    batch : 32320    loss : 0.00158    time :  5.748\n","epoch : 1    batch : 32360    loss : 0.00143    time :  5.733\n","epoch : 1    batch : 32400    loss : 0.00538    time :  5.728\n","epoch : 1    batch : 32440    loss : 0.00028    time :  5.725\n","epoch : 1    batch : 32480    loss : 0.00126    time :  5.743\n","epoch : 1    batch : 32520    loss : 0.00439    time :  5.733\n","epoch : 1    batch : 32560    loss : 0.00119    time :  5.725\n","epoch : 1    batch : 32600    loss : 0.00020    time :  5.715\n","epoch : 1    batch : 32640    loss : 0.01143    time :  5.749\n","epoch : 1    batch : 32680    loss : 0.00083    time :  5.716\n","epoch : 1    batch : 32720    loss : 0.00659    time :  5.724\n","epoch : 1    batch : 32760    loss : 0.00029    time :  5.704\n","epoch : 1    batch : 32800    loss : 0.00130    time :  5.746\n","epoch : 1    batch : 32840    loss : 0.00056    time :  5.729\n","epoch : 1    batch : 32880    loss : 0.00219    time :  5.733\n","epoch : 1    batch : 32920    loss : 0.00105    time :  5.714\n","epoch : 1    batch : 32960    loss : 0.00446    time :  6.030\n","epoch : 1    batch : 33000    loss : 0.00207    time :  5.715\n","epoch : 1    batch : 33040    loss : 0.00103    time :  5.760\n","epoch : 1    batch : 33080    loss : 0.00066    time :  5.720\n","epoch : 1    batch : 33120    loss : 0.00074    time :  5.724\n","epoch : 1    batch : 33160    loss : 0.00311    time :  5.718\n","epoch : 1    batch : 33200    loss : 0.00383    time :  5.757\n","epoch : 1    batch : 33240    loss : 0.00333    time :  5.728\n","epoch : 1    batch : 33280    loss : 0.00013    time :  5.745\n","epoch : 1    batch : 33320    loss : 0.00074    time :  5.712\n","epoch : 1    batch : 33360    loss : 0.00024    time :  5.746\n","epoch : 1    batch : 33400    loss : 0.00062    time :  5.707\n","epoch : 1    batch : 33440    loss : 0.00043    time :  5.727\n","epoch : 1    batch : 33480    loss : 0.00061    time :  5.732\n","epoch : 1    batch : 33520    loss : 0.00050    time :  5.734\n","epoch : 1    batch : 33560    loss : 0.00037    time :  5.714\n","epoch : 1    batch : 33600    loss : 0.00054    time :  5.732\n","epoch : 1    batch : 33640    loss : 0.00231    time :  5.723\n","epoch : 1    batch : 33680    loss : 0.00265    time :  5.718\n","epoch : 1    batch : 33720    loss : 0.00176    time :  5.746\n","epoch : 1    batch : 33760    loss : 0.00047    time :  5.749\n","epoch : 1    batch : 33800    loss : 0.00204    time :  5.736\n","epoch : 1    batch : 33840    loss : 0.02825    time :  5.725\n","epoch : 1    batch : 33880    loss : 0.00177    time :  5.740\n","epoch : 1    batch : 33920    loss : 0.00208    time :  5.742\n","epoch : 1    batch : 33960    loss : 0.00410    time :  5.738\n","epoch : 1    batch : 34000    loss : 0.00140    time :  5.725\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 1    batch : 34040    loss : 0.00195    time : 11.138\n","epoch : 1    batch : 34080    loss : 0.00134    time :  5.758\n","epoch : 1    batch : 34120    loss : 0.00011    time :  5.787\n","epoch : 1    batch : 34160    loss : 0.00043    time :  5.865\n","epoch : 1    batch : 34200    loss : 0.00046    time :  5.924\n","epoch : 1    batch : 34240    loss : 0.00195    time :  6.020\n","epoch : 1    batch : 34280    loss : 0.00062    time :  5.895\n","epoch : 1    batch : 34320    loss : 0.00115    time :  5.856\n","epoch : 1    batch : 34360    loss : 0.00167    time :  6.133\n","epoch : 1    batch : 34400    loss : 0.00009    time :  5.954\n","epoch : 1    batch : 34440    loss : 0.00102    time :  5.747\n","epoch : 1    batch : 34480    loss : 0.00147    time :  5.737\n","epoch : 1    batch : 34520    loss : 0.00093    time :  5.729\n","epoch : 1    batch : 34560    loss : 0.00154    time :  5.738\n","epoch : 1    batch : 34600    loss : 0.00047    time :  5.764\n","epoch : 1    batch : 34640    loss : 0.03687    time :  5.761\n","epoch : 1    batch : 34680    loss : 0.00204    time :  5.755\n","epoch : 1    batch : 34720    loss : 0.00107    time :  5.775\n","epoch : 1    batch : 34760    loss : 0.00337    time :  5.738\n","epoch : 1    batch : 34800    loss : 0.00156    time :  5.725\n","epoch : 1    batch : 34840    loss : 0.00173    time :  5.723\n","epoch : 1    batch : 34880    loss : 0.00461    time :  5.740\n","epoch : 1    batch : 34920    loss : 0.00284    time :  5.730\n","epoch : 1    batch : 34960    loss : 0.00179    time :  5.731\n","epoch : 1    batch : 35000    loss : 0.00269    time :  5.727\n","epoch : 1    batch : 35040    loss : 0.00079    time :  6.004\n","epoch : 1    batch : 35080    loss : 0.00010    time :  5.714\n","epoch : 1    batch : 35120    loss : 0.00223    time :  5.722\n","epoch : 1    batch : 35160    loss : 0.00045    time :  5.704\n","epoch : 1    batch : 35200    loss : 0.00467    time :  5.741\n","epoch : 1    batch : 35240    loss : 0.00250    time :  5.728\n","epoch : 1    batch : 35280    loss : 0.00145    time :  5.785\n","epoch : 1    batch : 35320    loss : 0.00550    time :  5.748\n","epoch : 1    batch : 35360    loss : 0.00788    time :  5.769\n","epoch : 1    batch : 35400    loss : 0.00398    time :  5.747\n","epoch : 1    batch : 35440    loss : 0.00118    time :  5.750\n","epoch : 1    batch : 35480    loss : 0.00160    time :  5.728\n","epoch : 1    batch : 35520    loss : 0.00056    time :  5.735\n","epoch : 1    batch : 35560    loss : 0.00142    time :  5.736\n","epoch : 1    batch : 35600    loss : 0.00095    time :  5.737\n","epoch : 1    batch : 35640    loss : 0.00112    time :  5.721\n","epoch : 1    batch : 35680    loss : 0.00250    time :  5.723\n","epoch : 1    batch : 35720    loss : 0.00097    time :  5.716\n","epoch : 1    batch : 35760    loss : 0.00157    time :  5.733\n","epoch : 1    batch : 35800    loss : 0.01431    time :  5.723\n","epoch : 1    batch : 35840    loss : 0.00257    time :  5.726\n","epoch : 1    batch : 35880    loss : 0.01217    time :  5.721\n","epoch : 1    batch : 35920    loss : 0.00202    time :  5.719\n","epoch : 1    batch : 35960    loss : 0.00102    time :  5.745\n","epoch : 1    batch : 36000    loss : 0.00343    time :  5.714\n","\n","Saving Model ...\n","Saving Finished\n","\n","########### START epoch : 2 ###########\n","Data Loading\n","data_torch-4000.pt Data Loading Time : 134.566\n","epoch : 2    batch :    40    loss : 0.00262    time : 140.366\n","epoch : 2    batch :    80    loss : 0.00512    time :  5.730\n","epoch : 2    batch :   120    loss : 0.00915    time :  5.728\n","epoch : 2    batch :   160    loss : 0.00098    time :  5.725\n","epoch : 2    batch :   200    loss : 0.00479    time :  5.755\n","epoch : 2    batch :   240    loss : 0.00410    time :  5.735\n","epoch : 2    batch :   280    loss : 0.00478    time :  5.754\n","epoch : 2    batch :   320    loss : 0.00266    time :  5.726\n","epoch : 2    batch :   360    loss : 0.01302    time :  5.734\n","epoch : 2    batch :   400    loss : 0.00481    time :  5.716\n","epoch : 2    batch :   440    loss : 0.00056    time :  5.734\n","epoch : 2    batch :   480    loss : 0.00177    time :  5.715\n","epoch : 2    batch :   520    loss : 0.00254    time :  6.008\n","epoch : 2    batch :   560    loss : 0.00575    time :  5.714\n","epoch : 2    batch :   600    loss : 0.00097    time :  5.724\n","epoch : 2    batch :   640    loss : 0.00071    time :  5.706\n","epoch : 2    batch :   680    loss : 0.00012    time :  5.730\n","epoch : 2    batch :   720    loss : 0.00050    time :  5.708\n","epoch : 2    batch :   760    loss : 0.00801    time :  5.723\n","epoch : 2    batch :   800    loss : 0.00038    time :  5.723\n","epoch : 2    batch :   840    loss : 0.00210    time :  5.753\n","epoch : 2    batch :   880    loss : 0.00307    time :  5.742\n","epoch : 2    batch :   920    loss : 0.00815    time :  5.754\n","epoch : 2    batch :   960    loss : 0.00094    time :  5.729\n","epoch : 2    batch :  1000    loss : 0.00543    time :  5.747\n","epoch : 2    batch :  1040    loss : 0.00315    time :  5.716\n","epoch : 2    batch :  1080    loss : 0.00317    time :  5.749\n","epoch : 2    batch :  1120    loss : 0.00622    time :  5.725\n","epoch : 2    batch :  1160    loss : 0.00160    time :  5.730\n","epoch : 2    batch :  1200    loss : 0.01534    time :  5.725\n","epoch : 2    batch :  1240    loss : 0.00085    time :  5.723\n","epoch : 2    batch :  1280    loss : 0.00003    time :  5.754\n","epoch : 2    batch :  1320    loss : 0.00151    time :  5.738\n","epoch : 2    batch :  1360    loss : 0.00224    time :  5.745\n","epoch : 2    batch :  1400    loss : 0.00118    time :  5.724\n","epoch : 2    batch :  1440    loss : 0.00433    time :  5.771\n","epoch : 2    batch :  1480    loss : 0.00050    time :  5.734\n","epoch : 2    batch :  1520    loss : 0.00108    time :  5.753\n","epoch : 2    batch :  1560    loss : 0.00307    time :  5.734\n","epoch : 2    batch :  1600    loss : 0.00044    time :  5.745\n","epoch : 2    batch :  1640    loss : 0.00303    time :  5.716\n","epoch : 2    batch :  1680    loss : 0.00365    time :  5.729\n","epoch : 2    batch :  1720    loss : 0.01169    time :  5.716\n","epoch : 2    batch :  1760    loss : 0.00146    time :  5.734\n","epoch : 2    batch :  1800    loss : 0.00349    time :  5.706\n","epoch : 2    batch :  1840    loss : 0.00011    time :  5.750\n","epoch : 2    batch :  1880    loss : 0.00103    time :  5.719\n","epoch : 2    batch :  1920    loss : 0.00015    time :  5.737\n","epoch : 2    batch :  1960    loss : 0.00044    time :  5.727\n","epoch : 2    batch :  2000    loss : 0.00527    time :  5.759\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch :  2040    loss : 0.00004    time : 12.174\n","epoch : 2    batch :  2080    loss : 0.00317    time :  5.736\n","epoch : 2    batch :  2120    loss : 0.00051    time :  5.745\n","epoch : 2    batch :  2160    loss : 0.00254    time :  5.804\n","epoch : 2    batch :  2200    loss : 0.00136    time :  5.790\n","epoch : 2    batch :  2240    loss : 0.00133    time :  5.774\n","epoch : 2    batch :  2280    loss : 0.00116    time :  5.775\n","epoch : 2    batch :  2320    loss : 0.00078    time :  5.752\n","epoch : 2    batch :  2360    loss : 0.00031    time :  5.855\n","epoch : 2    batch :  2400    loss : 0.00096    time :  5.732\n","epoch : 2    batch :  2440    loss : 0.00183    time :  5.769\n","epoch : 2    batch :  2480    loss : 0.00130    time :  5.718\n","epoch : 2    batch :  2520    loss : 0.00042    time :  5.739\n","epoch : 2    batch :  2560    loss : 0.00203    time :  5.719\n","epoch : 2    batch :  2600    loss : 0.00020    time :  5.718\n","epoch : 2    batch :  2640    loss : 0.00405    time :  5.725\n","epoch : 2    batch :  2680    loss : 0.00108    time :  5.720\n","epoch : 2    batch :  2720    loss : 0.00334    time :  5.717\n","epoch : 2    batch :  2760    loss : 0.00119    time :  5.964\n","epoch : 2    batch :  2800    loss : 0.00130    time :  5.726\n","epoch : 2    batch :  2840    loss : 0.00159    time :  5.732\n","epoch : 2    batch :  2880    loss : 0.00105    time :  5.721\n","epoch : 2    batch :  2920    loss : 0.00197    time :  5.729\n","epoch : 2    batch :  2960    loss : 0.00065    time :  5.732\n","epoch : 2    batch :  3000    loss : 0.00390    time :  5.732\n","epoch : 2    batch :  3040    loss : 0.00665    time :  5.763\n","epoch : 2    batch :  3080    loss : 0.02493    time :  5.740\n","epoch : 2    batch :  3120    loss : 0.00254    time :  5.739\n","epoch : 2    batch :  3160    loss : 0.01018    time :  5.750\n","epoch : 2    batch :  3200    loss : 0.00082    time :  5.719\n","epoch : 2    batch :  3240    loss : 0.00619    time :  5.724\n","epoch : 2    batch :  3280    loss : 0.00297    time :  5.730\n","epoch : 2    batch :  3320    loss : 0.00280    time :  5.727\n","epoch : 2    batch :  3360    loss : 0.00317    time :  5.729\n","epoch : 2    batch :  3400    loss : 0.00123    time :  5.729\n","epoch : 2    batch :  3440    loss : 0.00144    time :  5.729\n","epoch : 2    batch :  3480    loss : 0.00372    time :  5.708\n","epoch : 2    batch :  3520    loss : 0.00242    time :  5.739\n","epoch : 2    batch :  3560    loss : 0.00350    time :  5.725\n","epoch : 2    batch :  3600    loss : 0.00363    time :  5.725\n","epoch : 2    batch :  3640    loss : 0.00167    time :  5.726\n","epoch : 2    batch :  3680    loss : 0.00129    time :  5.736\n","epoch : 2    batch :  3720    loss : 0.00033    time :  5.708\n","epoch : 2    batch :  3760    loss : 0.00086    time :  5.718\n","epoch : 2    batch :  3800    loss : 0.00102    time :  5.716\n","epoch : 2    batch :  3840    loss : 0.00110    time :  5.726\n","epoch : 2    batch :  3880    loss : 0.00215    time :  5.722\n","epoch : 2    batch :  3920    loss : 0.00175    time :  5.734\n","epoch : 2    batch :  3960    loss : 0.00203    time :  5.721\n","epoch : 2    batch :  4000    loss : 0.00088    time :  5.726\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-8000.pt Data Loading Time : 135.417\n","epoch : 2    batch :  4040    loss : 0.00175    time : 146.557\n","epoch : 2    batch :  4080    loss : 0.00511    time :  5.707\n","epoch : 2    batch :  4120    loss : 0.00139    time :  5.755\n","epoch : 2    batch :  4160    loss : 0.00051    time :  5.724\n","epoch : 2    batch :  4200    loss : 0.00115    time :  5.970\n","epoch : 2    batch :  4240    loss : 0.00076    time :  5.728\n","epoch : 2    batch :  4280    loss : 0.00221    time :  5.737\n","epoch : 2    batch :  4320    loss : 0.00287    time :  5.720\n","epoch : 2    batch :  4360    loss : 0.00085    time :  5.739\n","epoch : 2    batch :  4400    loss : 0.00223    time :  5.719\n","epoch : 2    batch :  4440    loss : 0.00060    time :  5.765\n","epoch : 2    batch :  4480    loss : 0.00147    time :  5.741\n","epoch : 2    batch :  4520    loss : 0.00114    time :  5.760\n","epoch : 2    batch :  4560    loss : 0.00067    time :  5.728\n","epoch : 2    batch :  4600    loss : 0.00150    time :  5.733\n","epoch : 2    batch :  4640    loss : 0.00072    time :  5.705\n","epoch : 2    batch :  4680    loss : 0.00048    time :  5.720\n","epoch : 2    batch :  4720    loss : 0.00225    time :  5.710\n","epoch : 2    batch :  4760    loss : 0.00223    time :  5.733\n","epoch : 2    batch :  4800    loss : 0.00210    time :  5.736\n","epoch : 2    batch :  4840    loss : 0.00263    time :  5.742\n","epoch : 2    batch :  4880    loss : 0.00075    time :  5.737\n","epoch : 2    batch :  4920    loss : 0.00166    time :  5.751\n","epoch : 2    batch :  4960    loss : 0.00036    time :  5.727\n","epoch : 2    batch :  5000    loss : 0.00126    time :  5.740\n","epoch : 2    batch :  5040    loss : 0.00154    time :  5.724\n","epoch : 2    batch :  5080    loss : 0.00038    time :  5.742\n","epoch : 2    batch :  5120    loss : 0.00036    time :  5.743\n","epoch : 2    batch :  5160    loss : 0.00072    time :  5.757\n","epoch : 2    batch :  5200    loss : 0.00037    time :  5.736\n","epoch : 2    batch :  5240    loss : 0.00092    time :  5.767\n","epoch : 2    batch :  5280    loss : 0.00234    time :  5.734\n","epoch : 2    batch :  5320    loss : 0.00035    time :  5.754\n","epoch : 2    batch :  5360    loss : 0.00032    time :  5.746\n","epoch : 2    batch :  5400    loss : 0.00067    time :  5.746\n","epoch : 2    batch :  5440    loss : 0.00014    time :  5.730\n","epoch : 2    batch :  5480    loss : 0.01502    time :  5.732\n","epoch : 2    batch :  5520    loss : 0.00548    time :  5.742\n","epoch : 2    batch :  5560    loss : 0.00376    time :  5.727\n","epoch : 2    batch :  5600    loss : 0.00100    time :  5.733\n","epoch : 2    batch :  5640    loss : 0.00253    time :  5.733\n","epoch : 2    batch :  5680    loss : 0.00041    time :  5.741\n","epoch : 2    batch :  5720    loss : 0.00064    time :  5.726\n","epoch : 2    batch :  5760    loss : 0.00031    time :  5.727\n","epoch : 2    batch :  5800    loss : 0.00154    time :  5.715\n","epoch : 2    batch :  5840    loss : 0.00043    time :  5.735\n","epoch : 2    batch :  5880    loss : 0.00131    time :  5.730\n","epoch : 2    batch :  5920    loss : 0.00236    time :  5.748\n","epoch : 2    batch :  5960    loss : 0.00219    time :  5.740\n","epoch : 2    batch :  6000    loss : 0.00112    time :  5.742\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch :  6040    loss : 0.00058    time : 11.194\n","epoch : 2    batch :  6080    loss : 0.00030    time :  5.749\n","epoch : 2    batch :  6120    loss : 0.00450    time :  5.911\n","epoch : 2    batch :  6160    loss : 0.00073    time :  5.846\n","epoch : 2    batch :  6200    loss : 0.00179    time :  6.092\n","epoch : 2    batch :  6240    loss : 0.00084    time :  6.122\n","epoch : 2    batch :  6280    loss : 0.00143    time :  5.863\n","epoch : 2    batch :  6320    loss : 0.00075    time :  5.778\n","epoch : 2    batch :  6360    loss : 0.00157    time :  5.847\n","epoch : 2    batch :  6400    loss : 0.00121    time :  5.793\n","epoch : 2    batch :  6440    loss : 0.00260    time :  5.858\n","epoch : 2    batch :  6480    loss : 0.00085    time :  5.744\n","epoch : 2    batch :  6520    loss : 0.00139    time :  5.746\n","epoch : 2    batch :  6560    loss : 0.00638    time :  5.760\n","epoch : 2    batch :  6600    loss : 0.00093    time :  5.749\n","epoch : 2    batch :  6640    loss : 0.00899    time :  5.738\n","epoch : 2    batch :  6680    loss : 0.01412    time :  5.761\n","epoch : 2    batch :  6720    loss : 0.00119    time :  5.743\n","epoch : 2    batch :  6760    loss : 0.00298    time :  5.757\n","epoch : 2    batch :  6800    loss : 0.00427    time :  5.750\n","epoch : 2    batch :  6840    loss : 0.00258    time :  5.759\n","epoch : 2    batch :  6880    loss : 0.00093    time :  5.738\n","epoch : 2    batch :  6920    loss : 0.00096    time :  6.051\n","epoch : 2    batch :  6960    loss : 0.00285    time :  5.744\n","epoch : 2    batch :  7000    loss : 0.00073    time :  5.778\n","epoch : 2    batch :  7040    loss : 0.00144    time :  5.745\n","epoch : 2    batch :  7080    loss : 0.00081    time :  5.759\n","epoch : 2    batch :  7120    loss : 0.00032    time :  5.724\n","epoch : 2    batch :  7160    loss : 0.00089    time :  5.731\n","epoch : 2    batch :  7200    loss : 0.00088    time :  5.742\n","epoch : 2    batch :  7240    loss : 0.00521    time :  5.747\n","epoch : 2    batch :  7280    loss : 0.00055    time :  5.733\n","epoch : 2    batch :  7320    loss : 0.00202    time :  5.753\n","epoch : 2    batch :  7360    loss : 0.00199    time :  5.739\n","epoch : 2    batch :  7400    loss : 0.00063    time :  5.730\n","epoch : 2    batch :  7440    loss : 0.00125    time :  5.734\n","epoch : 2    batch :  7480    loss : 0.00093    time :  5.737\n","epoch : 2    batch :  7520    loss : 0.00030    time :  5.751\n","epoch : 2    batch :  7560    loss : 0.00048    time :  5.728\n","epoch : 2    batch :  7600    loss : 0.00088    time :  5.735\n","epoch : 2    batch :  7640    loss : 0.00017    time :  5.738\n","epoch : 2    batch :  7680    loss : 0.00008    time :  5.762\n","epoch : 2    batch :  7720    loss : 0.00033    time :  5.719\n","epoch : 2    batch :  7760    loss : 0.00011    time :  5.735\n","epoch : 2    batch :  7800    loss : 0.00050    time :  5.722\n","epoch : 2    batch :  7840    loss : 0.00160    time :  5.760\n","epoch : 2    batch :  7880    loss : 0.00003    time :  5.729\n","epoch : 2    batch :  7920    loss : 0.00026    time :  5.746\n","epoch : 2    batch :  7960    loss : 0.00041    time :  5.731\n","epoch : 2    batch :  8000    loss : 0.00043    time :  5.733\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-12000.pt Data Loading Time : 138.313\n","epoch : 2    batch :  8040    loss : 0.00023    time : 150.304\n","epoch : 2    batch :  8080    loss : 0.00042    time :  5.746\n","epoch : 2    batch :  8120    loss : 0.00073    time :  5.737\n","epoch : 2    batch :  8160    loss : 0.00137    time :  5.761\n","epoch : 2    batch :  8200    loss : 0.00022    time :  5.726\n","epoch : 2    batch :  8240    loss : 0.00072    time :  5.751\n","epoch : 2    batch :  8280    loss : 0.00021    time :  5.734\n","epoch : 2    batch :  8320    loss : 0.00046    time :  5.734\n","epoch : 2    batch :  8360    loss : 0.00045    time :  5.737\n","epoch : 2    batch :  8400    loss : 0.00579    time :  5.748\n","epoch : 2    batch :  8440    loss : 0.00147    time :  5.714\n","epoch : 2    batch :  8480    loss : 0.00589    time :  5.736\n","epoch : 2    batch :  8520    loss : 0.00136    time :  5.740\n","epoch : 2    batch :  8560    loss : 0.00298    time :  5.741\n","epoch : 2    batch :  8600    loss : 0.00232    time :  5.731\n","epoch : 2    batch :  8640    loss : 0.00055    time :  5.761\n","epoch : 2    batch :  8680    loss : 0.00005    time :  5.740\n","epoch : 2    batch :  8720    loss : 0.00137    time :  5.775\n","epoch : 2    batch :  8760    loss : 0.00678    time :  5.748\n","epoch : 2    batch :  8800    loss : 0.00128    time :  5.742\n","epoch : 2    batch :  8840    loss : 0.00178    time :  5.716\n","epoch : 2    batch :  8880    loss : 0.00122    time :  5.755\n","epoch : 2    batch :  8920    loss : 0.00026    time :  5.737\n","epoch : 2    batch :  8960    loss : 0.00026    time :  5.768\n","epoch : 2    batch :  9000    loss : 0.00029    time :  5.723\n","epoch : 2    batch :  9040    loss : 0.00024    time :  5.756\n","epoch : 2    batch :  9080    loss : 0.00103    time :  5.733\n","epoch : 2    batch :  9120    loss : 0.00030    time :  5.770\n","epoch : 2    batch :  9160    loss : 0.00079    time :  5.728\n","epoch : 2    batch :  9200    loss : 0.00048    time :  5.745\n","epoch : 2    batch :  9240    loss : 0.00151    time :  5.738\n","epoch : 2    batch :  9280    loss : 0.00007    time :  5.742\n","epoch : 2    batch :  9320    loss : 0.00029    time :  5.752\n","epoch : 2    batch :  9360    loss : 0.00024    time :  5.755\n","epoch : 2    batch :  9400    loss : 0.00041    time :  5.761\n","epoch : 2    batch :  9440    loss : 0.00007    time :  5.759\n","epoch : 2    batch :  9480    loss : 0.00013    time :  5.744\n","epoch : 2    batch :  9520    loss : 0.00022    time :  5.753\n","epoch : 2    batch :  9560    loss : 0.00029    time :  5.746\n","epoch : 2    batch :  9600    loss : 0.00093    time :  5.744\n","epoch : 2    batch :  9640    loss : 0.00293    time :  6.000\n","epoch : 2    batch :  9680    loss : 0.00006    time :  5.739\n","epoch : 2    batch :  9720    loss : 0.00075    time :  5.754\n","epoch : 2    batch :  9760    loss : 0.00076    time :  5.743\n","epoch : 2    batch :  9800    loss : 0.00032    time :  5.736\n","epoch : 2    batch :  9840    loss : 0.00116    time :  5.724\n","epoch : 2    batch :  9880    loss : 0.00031    time :  5.738\n","epoch : 2    batch :  9920    loss : 0.01123    time :  5.738\n","epoch : 2    batch :  9960    loss : 0.01068    time :  5.734\n","epoch : 2    batch : 10000    loss : 0.00145    time :  5.722\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch : 10040    loss : 0.00114    time : 11.063\n","epoch : 2    batch : 10080    loss : 0.00445    time :  5.784\n","epoch : 2    batch : 10120    loss : 0.00435    time :  5.805\n","epoch : 2    batch : 10160    loss : 0.00277    time :  5.865\n","epoch : 2    batch : 10200    loss : 0.00163    time :  5.752\n","epoch : 2    batch : 10240    loss : 0.00070    time :  5.787\n","epoch : 2    batch : 10280    loss : 0.00375    time :  5.799\n","epoch : 2    batch : 10320    loss : 0.00073    time :  5.818\n","epoch : 2    batch : 10360    loss : 0.00255    time :  5.766\n","epoch : 2    batch : 10400    loss : 0.00017    time :  5.763\n","epoch : 2    batch : 10440    loss : 0.00020    time :  5.761\n","epoch : 2    batch : 10480    loss : 0.00015    time :  5.835\n","epoch : 2    batch : 10520    loss : 0.00110    time :  5.775\n","epoch : 2    batch : 10560    loss : 0.00200    time :  5.757\n","epoch : 2    batch : 10600    loss : 0.00076    time :  5.744\n","epoch : 2    batch : 10640    loss : 0.00010    time :  5.752\n","epoch : 2    batch : 10680    loss : 0.00118    time :  5.758\n","epoch : 2    batch : 10720    loss : 0.00080    time :  5.780\n","epoch : 2    batch : 10760    loss : 0.00983    time :  5.795\n","epoch : 2    batch : 10800    loss : 0.00117    time :  5.799\n","epoch : 2    batch : 10840    loss : 0.00141    time :  5.774\n","epoch : 2    batch : 10880    loss : 0.00022    time :  5.772\n","epoch : 2    batch : 10920    loss : 0.00051    time :  5.732\n","epoch : 2    batch : 10960    loss : 0.00137    time :  5.738\n","epoch : 2    batch : 11000    loss : 0.00014    time :  5.771\n","epoch : 2    batch : 11040    loss : 0.00122    time :  5.777\n","epoch : 2    batch : 11080    loss : 0.00133    time :  5.752\n","epoch : 2    batch : 11120    loss : 0.00084    time :  5.741\n","epoch : 2    batch : 11160    loss : 0.00013    time :  5.745\n","epoch : 2    batch : 11200    loss : 0.00018    time :  5.731\n","epoch : 2    batch : 11240    loss : 0.00058    time :  5.732\n","epoch : 2    batch : 11280    loss : 0.00135    time :  5.742\n","epoch : 2    batch : 11320    loss : 0.01485    time :  5.720\n","epoch : 2    batch : 11360    loss : 0.01754    time :  5.731\n","epoch : 2    batch : 11400    loss : 0.02275    time :  5.739\n","epoch : 2    batch : 11440    loss : 0.02015    time :  5.721\n","epoch : 2    batch : 11480    loss : 0.01212    time :  5.732\n","epoch : 2    batch : 11520    loss : 0.03372    time :  5.736\n","epoch : 2    batch : 11560    loss : 0.01777    time :  5.748\n","epoch : 2    batch : 11600    loss : 0.03172    time :  5.715\n","epoch : 2    batch : 11640    loss : 0.02069    time :  5.743\n","epoch : 2    batch : 11680    loss : 0.00767    time :  5.739\n","epoch : 2    batch : 11720    loss : 0.01104    time :  5.748\n","epoch : 2    batch : 11760    loss : 0.00235    time :  5.711\n","epoch : 2    batch : 11800    loss : 0.00331    time :  5.722\n","epoch : 2    batch : 11840    loss : 0.00148    time :  5.712\n","epoch : 2    batch : 11880    loss : 0.00050    time :  5.738\n","epoch : 2    batch : 11920    loss : 0.00652    time :  5.698\n","epoch : 2    batch : 11960    loss : 0.00313    time :  5.717\n","epoch : 2    batch : 12000    loss : 0.00156    time :  5.721\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-16000.pt Data Loading Time : 128.293\n","epoch : 2    batch : 12040    loss : 0.00165    time : 139.582\n","epoch : 2    batch : 12080    loss : 0.00119    time :  5.739\n","epoch : 2    batch : 12120    loss : 0.00064    time :  5.773\n","epoch : 2    batch : 12160    loss : 0.00093    time :  5.771\n","epoch : 2    batch : 12200    loss : 0.00226    time :  5.766\n","epoch : 2    batch : 12240    loss : 0.00236    time :  5.765\n","epoch : 2    batch : 12280    loss : 0.00358    time :  5.757\n","epoch : 2    batch : 12320    loss : 0.00614    time :  5.745\n","epoch : 2    batch : 12360    loss : 0.00554    time :  5.738\n","epoch : 2    batch : 12400    loss : 0.00336    time :  5.743\n","epoch : 2    batch : 12440    loss : 0.00367    time :  5.780\n","epoch : 2    batch : 12480    loss : 0.00141    time :  5.750\n","epoch : 2    batch : 12520    loss : 0.00033    time :  5.747\n","epoch : 2    batch : 12560    loss : 0.00059    time :  5.725\n","epoch : 2    batch : 12600    loss : 0.00073    time :  5.769\n","epoch : 2    batch : 12640    loss : 0.00014    time :  5.753\n","epoch : 2    batch : 12680    loss : 0.00052    time :  5.787\n","epoch : 2    batch : 12720    loss : 0.00134    time :  5.774\n","epoch : 2    batch : 12760    loss : 0.00038    time :  5.779\n","epoch : 2    batch : 12800    loss : 0.00101    time :  5.721\n","epoch : 2    batch : 12840    loss : 0.00413    time :  5.760\n","epoch : 2    batch : 12880    loss : 0.00125    time :  5.826\n","epoch : 2    batch : 12920    loss : 0.00011    time :  5.911\n","epoch : 2    batch : 12960    loss : 0.00258    time :  5.891\n","epoch : 2    batch : 13000    loss : 0.00170    time :  5.937\n","epoch : 2    batch : 13040    loss : 0.00124    time :  5.895\n","epoch : 2    batch : 13080    loss : 0.00789    time :  5.946\n","epoch : 2    batch : 13120    loss : 0.00431    time :  5.894\n","epoch : 2    batch : 13160    loss : 0.00566    time :  5.985\n","epoch : 2    batch : 13200    loss : 0.00130    time :  6.173\n","epoch : 2    batch : 13240    loss : 0.00004    time :  5.938\n","epoch : 2    batch : 13280    loss : 0.00035    time :  5.914\n","epoch : 2    batch : 13320    loss : 0.00061    time :  5.938\n","epoch : 2    batch : 13360    loss : 0.00031    time :  5.875\n","epoch : 2    batch : 13400    loss : 0.00012    time :  5.894\n","epoch : 2    batch : 13440    loss : 0.00004    time :  5.758\n","epoch : 2    batch : 13480    loss : 0.00120    time :  5.777\n","epoch : 2    batch : 13520    loss : 0.00189    time :  5.745\n","epoch : 2    batch : 13560    loss : 0.00054    time :  5.747\n","epoch : 2    batch : 13600    loss : 0.00026    time :  5.732\n","epoch : 2    batch : 13640    loss : 0.00019    time :  5.737\n","epoch : 2    batch : 13680    loss : 0.00005    time :  5.730\n","epoch : 2    batch : 13720    loss : 0.00027    time :  5.742\n","epoch : 2    batch : 13760    loss : 0.00074    time :  5.735\n","epoch : 2    batch : 13800    loss : 0.00031    time :  5.794\n","epoch : 2    batch : 13840    loss : 0.00605    time :  5.771\n","epoch : 2    batch : 13880    loss : 0.00230    time :  5.754\n","epoch : 2    batch : 13920    loss : 0.00069    time :  5.772\n","epoch : 2    batch : 13960    loss : 0.00250    time :  5.728\n","epoch : 2    batch : 14000    loss : 0.00354    time :  5.747\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch : 14040    loss : 0.00029    time : 11.370\n","epoch : 2    batch : 14080    loss : 0.00421    time :  5.768\n","epoch : 2    batch : 14120    loss : 0.00034    time :  5.790\n","epoch : 2    batch : 14160    loss : 0.00082    time :  5.780\n","epoch : 2    batch : 14200    loss : 0.00151    time :  5.793\n","epoch : 2    batch : 14240    loss : 0.00099    time :  5.979\n","epoch : 2    batch : 14280    loss : 0.00100    time :  5.996\n","epoch : 2    batch : 14320    loss : 0.00125    time :  5.993\n","epoch : 2    batch : 14360    loss : 0.00120    time :  6.022\n","epoch : 2    batch : 14400    loss : 0.00141    time :  5.994\n","epoch : 2    batch : 14440    loss : 0.00014    time :  6.035\n","epoch : 2    batch : 14480    loss : 0.00087    time :  5.994\n","epoch : 2    batch : 14520    loss : 0.00040    time :  5.997\n","epoch : 2    batch : 14560    loss : 0.00161    time :  5.976\n","epoch : 2    batch : 14600    loss : 0.00077    time :  5.942\n","epoch : 2    batch : 14640    loss : 0.00030    time :  5.937\n","epoch : 2    batch : 14680    loss : 0.00100    time :  5.954\n","epoch : 2    batch : 14720    loss : 0.00051    time :  5.957\n","epoch : 2    batch : 14760    loss : 0.00062    time :  5.987\n","epoch : 2    batch : 14800    loss : 0.00023    time :  5.964\n","epoch : 2    batch : 14840    loss : 0.00045    time :  5.967\n","epoch : 2    batch : 14880    loss : 0.00113    time :  5.939\n","epoch : 2    batch : 14920    loss : 0.00146    time :  5.844\n","epoch : 2    batch : 14960    loss : 0.00066    time :  5.960\n","epoch : 2    batch : 15000    loss : 0.00100    time :  5.984\n","epoch : 2    batch : 15040    loss : 0.00082    time :  5.962\n","epoch : 2    batch : 15080    loss : 0.00027    time :  5.966\n","epoch : 2    batch : 15120    loss : 0.00042    time :  5.955\n","epoch : 2    batch : 15160    loss : 0.00172    time :  5.978\n","epoch : 2    batch : 15200    loss : 0.00066    time :  5.976\n","epoch : 2    batch : 15240    loss : 0.00043    time :  5.972\n","epoch : 2    batch : 15280    loss : 0.00083    time :  5.967\n","epoch : 2    batch : 15320    loss : 0.00648    time :  5.954\n","epoch : 2    batch : 15360    loss : 0.00224    time :  5.956\n","epoch : 2    batch : 15400    loss : 0.00123    time :  5.973\n","epoch : 2    batch : 15440    loss : 0.00463    time :  5.961\n","epoch : 2    batch : 15480    loss : 0.00130    time :  6.014\n","epoch : 2    batch : 15520    loss : 0.00188    time :  5.983\n","epoch : 2    batch : 15560    loss : 0.00041    time :  5.990\n","epoch : 2    batch : 15600    loss : 0.00106    time :  5.989\n","epoch : 2    batch : 15640    loss : 0.00223    time :  5.985\n","epoch : 2    batch : 15680    loss : 0.00320    time :  5.971\n","epoch : 2    batch : 15720    loss : 0.00028    time :  5.943\n","epoch : 2    batch : 15760    loss : 0.00240    time :  5.945\n","epoch : 2    batch : 15800    loss : 0.00081    time :  5.886\n","epoch : 2    batch : 15840    loss : 0.00069    time :  5.754\n","epoch : 2    batch : 15880    loss : 0.00090    time :  6.069\n","epoch : 2    batch : 15920    loss : 0.00025    time :  5.744\n","epoch : 2    batch : 15960    loss : 0.00016    time :  5.760\n","epoch : 2    batch : 16000    loss : 0.02430    time :  5.755\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-20000.pt Data Loading Time : 139.820\n","epoch : 2    batch : 16040    loss : 0.00016    time : 152.381\n","epoch : 2    batch : 16080    loss : 0.00085    time :  5.781\n","epoch : 2    batch : 16120    loss : 0.00036    time :  5.815\n","epoch : 2    batch : 16160    loss : 0.00030    time :  5.836\n","epoch : 2    batch : 16200    loss : 0.00025    time :  5.827\n","epoch : 2    batch : 16240    loss : 0.00036    time :  5.842\n","epoch : 2    batch : 16280    loss : 0.00040    time :  5.816\n","epoch : 2    batch : 16320    loss : 0.00014    time :  5.835\n","epoch : 2    batch : 16360    loss : 0.00068    time :  5.797\n","epoch : 2    batch : 16400    loss : 0.00145    time :  5.817\n","epoch : 2    batch : 16440    loss : 0.00405    time :  5.849\n","epoch : 2    batch : 16480    loss : 0.00063    time :  5.890\n","epoch : 2    batch : 16520    loss : 0.00269    time :  5.856\n","epoch : 2    batch : 16560    loss : 0.00184    time :  5.872\n","epoch : 2    batch : 16600    loss : 0.00054    time :  5.850\n","epoch : 2    batch : 16640    loss : 0.00033    time :  5.836\n","epoch : 2    batch : 16680    loss : 0.00045    time :  5.815\n","epoch : 2    batch : 16720    loss : 0.00030    time :  5.818\n","epoch : 2    batch : 16760    loss : 0.00079    time :  5.806\n","epoch : 2    batch : 16800    loss : 0.00108    time :  5.830\n","epoch : 2    batch : 16840    loss : 0.01649    time :  5.813\n","epoch : 2    batch : 16880    loss : 0.00420    time :  5.833\n","epoch : 2    batch : 16920    loss : 0.00474    time :  5.831\n","epoch : 2    batch : 16960    loss : 0.00116    time :  5.852\n","epoch : 2    batch : 17000    loss : 0.00233    time :  5.828\n","epoch : 2    batch : 17040    loss : 0.00602    time :  5.851\n","epoch : 2    batch : 17080    loss : 0.00061    time :  5.849\n","epoch : 2    batch : 17120    loss : 0.00029    time :  5.834\n","epoch : 2    batch : 17160    loss : 0.00123    time :  5.857\n","epoch : 2    batch : 17200    loss : 0.00053    time :  6.172\n","epoch : 2    batch : 17240    loss : 0.00023    time :  5.831\n","epoch : 2    batch : 17280    loss : 0.00005    time :  5.846\n","epoch : 2    batch : 17320    loss : 0.00196    time :  5.814\n","epoch : 2    batch : 17360    loss : 0.00051    time :  5.820\n","epoch : 2    batch : 17400    loss : 0.00008    time :  5.815\n","epoch : 2    batch : 17440    loss : 0.00120    time :  5.845\n","epoch : 2    batch : 17480    loss : 0.00109    time :  5.884\n","epoch : 2    batch : 17520    loss : 0.00048    time :  5.896\n","epoch : 2    batch : 17560    loss : 0.00531    time :  5.904\n","epoch : 2    batch : 17600    loss : 0.00208    time :  5.892\n","epoch : 2    batch : 17640    loss : 0.00048    time :  5.891\n","epoch : 2    batch : 17680    loss : 0.00280    time :  5.885\n","epoch : 2    batch : 17720    loss : 0.00141    time :  5.868\n","epoch : 2    batch : 17760    loss : 0.00092    time :  5.853\n","epoch : 2    batch : 17800    loss : 0.00407    time :  5.842\n","epoch : 2    batch : 17840    loss : 0.00096    time :  5.805\n","epoch : 2    batch : 17880    loss : 0.00305    time :  5.780\n","epoch : 2    batch : 17920    loss : 0.00818    time :  5.762\n","epoch : 2    batch : 17960    loss : 0.00012    time :  5.784\n","epoch : 2    batch : 18000    loss : 0.00067    time :  5.739\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch : 18040    loss : 0.00164    time : 11.251\n","epoch : 2    batch : 18080    loss : 0.00026    time :  5.790\n","epoch : 2    batch : 18120    loss : 0.00063    time :  5.754\n","epoch : 2    batch : 18160    loss : 0.00040    time :  5.785\n","epoch : 2    batch : 18200    loss : 0.00087    time :  5.783\n","epoch : 2    batch : 18240    loss : 0.00042    time :  5.793\n","epoch : 2    batch : 18280    loss : 0.00065    time :  5.750\n","epoch : 2    batch : 18320    loss : 0.00054    time :  5.802\n","epoch : 2    batch : 18360    loss : 0.00353    time :  5.775\n","epoch : 2    batch : 18400    loss : 0.00111    time :  5.827\n","epoch : 2    batch : 18440    loss : 0.00184    time :  5.860\n","epoch : 2    batch : 18480    loss : 0.00023    time :  5.772\n","epoch : 2    batch : 18520    loss : 0.00051    time :  5.766\n","epoch : 2    batch : 18560    loss : 0.00077    time :  5.769\n","epoch : 2    batch : 18600    loss : 0.00033    time :  5.719\n","epoch : 2    batch : 18640    loss : 0.00019    time :  5.717\n","epoch : 2    batch : 18680    loss : 0.00141    time :  5.726\n","epoch : 2    batch : 18720    loss : 0.00062    time :  5.739\n","epoch : 2    batch : 18760    loss : 0.00097    time :  5.721\n","epoch : 2    batch : 18800    loss : 0.00143    time :  5.735\n","epoch : 2    batch : 18840    loss : 0.00123    time :  5.720\n","epoch : 2    batch : 18880    loss : 0.00023    time :  5.735\n","epoch : 2    batch : 18920    loss : 0.00338    time :  5.717\n","epoch : 2    batch : 18960    loss : 0.00186    time :  5.733\n","epoch : 2    batch : 19000    loss : 0.01113    time :  5.731\n","epoch : 2    batch : 19040    loss : 0.00659    time :  5.741\n","epoch : 2    batch : 19080    loss : 0.00438    time :  5.710\n","epoch : 2    batch : 19120    loss : 0.00126    time :  5.736\n","epoch : 2    batch : 19160    loss : 0.01274    time :  5.712\n","epoch : 2    batch : 19200    loss : 0.00232    time :  5.740\n","epoch : 2    batch : 19240    loss : 0.00091    time :  5.713\n","epoch : 2    batch : 19280    loss : 0.00129    time :  5.737\n","epoch : 2    batch : 19320    loss : 0.00090    time :  5.715\n","epoch : 2    batch : 19360    loss : 0.00025    time :  5.733\n","epoch : 2    batch : 19400    loss : 0.00086    time :  5.729\n","epoch : 2    batch : 19440    loss : 0.00101    time :  6.032\n","epoch : 2    batch : 19480    loss : 0.00028    time :  5.751\n","epoch : 2    batch : 19520    loss : 0.00415    time :  5.762\n","epoch : 2    batch : 19560    loss : 0.00027    time :  5.766\n","epoch : 2    batch : 19600    loss : 0.00045    time :  5.742\n","epoch : 2    batch : 19640    loss : 0.00017    time :  5.750\n","epoch : 2    batch : 19680    loss : 0.00035    time :  5.729\n","epoch : 2    batch : 19720    loss : 0.00004    time :  5.736\n","epoch : 2    batch : 19760    loss : 0.00004    time :  5.717\n","epoch : 2    batch : 19800    loss : 0.00019    time :  5.775\n","epoch : 2    batch : 19840    loss : 0.00036    time :  5.765\n","epoch : 2    batch : 19880    loss : 0.00059    time :  5.817\n","epoch : 2    batch : 19920    loss : 0.00115    time :  5.806\n","epoch : 2    batch : 19960    loss : 0.00045    time :  5.851\n","epoch : 2    batch : 20000    loss : 0.00147    time :  5.764\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-24000.pt Data Loading Time : 132.032\n","epoch : 2    batch : 20040    loss : 0.00803    time : 143.114\n","epoch : 2    batch : 20080    loss : 0.00045    time :  5.722\n","epoch : 2    batch : 20120    loss : 0.00072    time :  5.715\n","epoch : 2    batch : 20160    loss : 0.00056    time :  5.725\n","epoch : 2    batch : 20200    loss : 0.00223    time :  5.737\n","epoch : 2    batch : 20240    loss : 0.00042    time :  5.762\n","epoch : 2    batch : 20280    loss : 0.00160    time :  5.754\n","epoch : 2    batch : 20320    loss : 0.00201    time :  5.743\n","epoch : 2    batch : 20360    loss : 0.00211    time :  5.720\n","epoch : 2    batch : 20400    loss : 0.00299    time :  5.726\n","epoch : 2    batch : 20440    loss : 0.00105    time :  5.757\n","epoch : 2    batch : 20480    loss : 0.00321    time :  5.749\n","epoch : 2    batch : 20520    loss : 0.00137    time :  5.716\n","epoch : 2    batch : 20560    loss : 0.00010    time :  5.738\n","epoch : 2    batch : 20600    loss : 0.00063    time :  5.725\n","epoch : 2    batch : 20640    loss : 0.00087    time :  5.731\n","epoch : 2    batch : 20680    loss : 0.00029    time :  5.731\n","epoch : 2    batch : 20720    loss : 0.00120    time :  5.732\n","epoch : 2    batch : 20760    loss : 0.00023    time :  5.751\n","epoch : 2    batch : 20800    loss : 0.00654    time :  5.730\n","epoch : 2    batch : 20840    loss : 0.00147    time :  5.724\n","epoch : 2    batch : 20880    loss : 0.00007    time :  5.725\n","epoch : 2    batch : 20920    loss : 0.00775    time :  5.727\n","epoch : 2    batch : 20960    loss : 0.01150    time :  5.999\n","epoch : 2    batch : 21000    loss : 0.00336    time :  5.727\n","epoch : 2    batch : 21040    loss : 0.01163    time :  5.728\n","epoch : 2    batch : 21080    loss : 0.00021    time :  5.718\n","epoch : 2    batch : 21120    loss : 0.00184    time :  5.720\n","epoch : 2    batch : 21160    loss : 0.00190    time :  5.747\n","epoch : 2    batch : 21200    loss : 0.00057    time :  5.771\n","epoch : 2    batch : 21240    loss : 0.00108    time :  5.772\n","epoch : 2    batch : 21280    loss : 0.00021    time :  5.749\n","epoch : 2    batch : 21320    loss : 0.00025    time :  5.771\n","epoch : 2    batch : 21360    loss : 0.00115    time :  5.725\n","epoch : 2    batch : 21400    loss : 0.00069    time :  5.749\n","epoch : 2    batch : 21440    loss : 0.03908    time :  5.731\n","epoch : 2    batch : 21480    loss : 0.01898    time :  5.741\n","epoch : 2    batch : 21520    loss : 0.00305    time :  5.746\n","epoch : 2    batch : 21560    loss : 0.00473    time :  5.758\n","epoch : 2    batch : 21600    loss : 0.00349    time :  5.741\n","epoch : 2    batch : 21640    loss : 0.00397    time :  5.762\n","epoch : 2    batch : 21680    loss : 0.00352    time :  5.737\n","epoch : 2    batch : 21720    loss : 0.00171    time :  5.737\n","epoch : 2    batch : 21760    loss : 0.00011    time :  5.716\n","epoch : 2    batch : 21800    loss : 0.00260    time :  5.750\n","epoch : 2    batch : 21840    loss : 0.00052    time :  5.738\n","epoch : 2    batch : 21880    loss : 0.00160    time :  5.757\n","epoch : 2    batch : 21920    loss : 0.00212    time :  5.744\n","epoch : 2    batch : 21960    loss : 0.00101    time :  5.752\n","epoch : 2    batch : 22000    loss : 0.00250    time :  5.734\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch : 22040    loss : 0.00340    time : 11.214\n","epoch : 2    batch : 22080    loss : 0.00015    time :  5.779\n","epoch : 2    batch : 22120    loss : 0.00459    time :  5.843\n","epoch : 2    batch : 22160    loss : 0.00074    time :  5.815\n","epoch : 2    batch : 22200    loss : 0.00235    time :  5.767\n","epoch : 2    batch : 22240    loss : 0.00573    time :  5.839\n","epoch : 2    batch : 22280    loss : 0.00317    time :  5.755\n","epoch : 2    batch : 22320    loss : 0.01975    time :  5.757\n","epoch : 2    batch : 22360    loss : 0.00285    time :  5.774\n","epoch : 2    batch : 22400    loss : 0.00211    time :  5.790\n","epoch : 2    batch : 22440    loss : 0.01750    time :  5.841\n","epoch : 2    batch : 22480    loss : 0.00613    time :  5.751\n","epoch : 2    batch : 22520    loss : 0.00305    time :  5.737\n","epoch : 2    batch : 22560    loss : 0.01998    time :  5.758\n","epoch : 2    batch : 22600    loss : 0.00227    time :  5.755\n","epoch : 2    batch : 22640    loss : 0.03493    time :  5.753\n","epoch : 2    batch : 22680    loss : 0.00257    time :  5.799\n","epoch : 2    batch : 22720    loss : 0.00592    time :  5.764\n","epoch : 2    batch : 22760    loss : 0.00373    time :  5.738\n","epoch : 2    batch : 22800    loss : 0.00061    time :  5.737\n","epoch : 2    batch : 22840    loss : 0.00532    time :  5.752\n","epoch : 2    batch : 22880    loss : 0.00236    time :  5.762\n","epoch : 2    batch : 22920    loss : 0.00114    time :  5.736\n","epoch : 2    batch : 22960    loss : 0.00282    time :  5.735\n","epoch : 2    batch : 23000    loss : 0.00296    time :  5.966\n","epoch : 2    batch : 23040    loss : 0.00169    time :  5.746\n","epoch : 2    batch : 23080    loss : 0.00852    time :  5.765\n","epoch : 2    batch : 23120    loss : 0.00185    time :  5.779\n","epoch : 2    batch : 23160    loss : 0.03805    time :  5.792\n","epoch : 2    batch : 23200    loss : 0.00372    time :  5.770\n","epoch : 2    batch : 23240    loss : 0.00107    time :  5.778\n","epoch : 2    batch : 23280    loss : 0.00309    time :  5.736\n","epoch : 2    batch : 23320    loss : 0.00090    time :  5.759\n","epoch : 2    batch : 23360    loss : 0.00160    time :  5.731\n","epoch : 2    batch : 23400    loss : 0.00058    time :  5.764\n","epoch : 2    batch : 23440    loss : 0.00022    time :  5.734\n","epoch : 2    batch : 23480    loss : 0.00374    time :  5.758\n","epoch : 2    batch : 23520    loss : 0.00249    time :  5.730\n","epoch : 2    batch : 23560    loss : 0.00063    time :  5.749\n","epoch : 2    batch : 23600    loss : 0.00161    time :  5.741\n","epoch : 2    batch : 23640    loss : 0.00949    time :  5.755\n","epoch : 2    batch : 23680    loss : 0.00524    time :  5.726\n","epoch : 2    batch : 23720    loss : 0.00096    time :  5.739\n","epoch : 2    batch : 23760    loss : 0.00245    time :  5.721\n","epoch : 2    batch : 23800    loss : 0.00057    time :  5.729\n","epoch : 2    batch : 23840    loss : 0.00028    time :  5.716\n","epoch : 2    batch : 23880    loss : 0.00044    time :  5.730\n","epoch : 2    batch : 23920    loss : 0.00013    time :  5.709\n","epoch : 2    batch : 23960    loss : 0.00085    time :  5.725\n","epoch : 2    batch : 24000    loss : 0.00026    time :  5.715\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-28000.pt Data Loading Time : 126.128\n","epoch : 2    batch : 24040    loss : 0.00009    time : 137.216\n","epoch : 2    batch : 24080    loss : 0.00001    time :  5.722\n","epoch : 2    batch : 24120    loss : 0.00050    time :  5.739\n","epoch : 2    batch : 24160    loss : 0.00049    time :  5.724\n","epoch : 2    batch : 24200    loss : 0.00051    time :  5.741\n","epoch : 2    batch : 24240    loss : 0.00012    time :  5.722\n","epoch : 2    batch : 24280    loss : 0.00085    time :  5.724\n","epoch : 2    batch : 24320    loss : 0.00262    time :  5.767\n","epoch : 2    batch : 24360    loss : 0.00055    time :  5.991\n","epoch : 2    batch : 24400    loss : 0.00257    time :  5.749\n","epoch : 2    batch : 24440    loss : 0.00014    time :  5.745\n","epoch : 2    batch : 24480    loss : 0.00094    time :  5.750\n","epoch : 2    batch : 24520    loss : 0.00061    time :  5.746\n","epoch : 2    batch : 24560    loss : 0.00594    time :  5.751\n","epoch : 2    batch : 24600    loss : 0.00424    time :  5.725\n","epoch : 2    batch : 24640    loss : 0.00033    time :  5.741\n","epoch : 2    batch : 24680    loss : 0.00091    time :  5.743\n","epoch : 2    batch : 24720    loss : 0.00018    time :  5.758\n","epoch : 2    batch : 24760    loss : 0.00107    time :  5.741\n","epoch : 2    batch : 24800    loss : 0.00022    time :  5.760\n","epoch : 2    batch : 24840    loss : 0.00103    time :  5.733\n","epoch : 2    batch : 24880    loss : 0.00039    time :  5.744\n","epoch : 2    batch : 24920    loss : 0.00033    time :  5.741\n","epoch : 2    batch : 24960    loss : 0.00158    time :  5.775\n","epoch : 2    batch : 25000    loss : 0.00023    time :  5.733\n","epoch : 2    batch : 25040    loss : 0.00077    time :  5.755\n","epoch : 2    batch : 25080    loss : 0.00012    time :  5.734\n","epoch : 2    batch : 25120    loss : 0.00008    time :  5.749\n","epoch : 2    batch : 25160    loss : 0.00025    time :  5.734\n","epoch : 2    batch : 25200    loss : 0.00026    time :  5.750\n","epoch : 2    batch : 25240    loss : 0.00038    time :  5.732\n","epoch : 2    batch : 25280    loss : 0.00058    time :  5.752\n","epoch : 2    batch : 25320    loss : 0.00152    time :  5.717\n","epoch : 2    batch : 25360    loss : 0.00051    time :  5.749\n","epoch : 2    batch : 25400    loss : 0.00063    time :  5.729\n","epoch : 2    batch : 25440    loss : 0.00131    time :  5.734\n","epoch : 2    batch : 25480    loss : 0.00033    time :  5.713\n","epoch : 2    batch : 25520    loss : 0.00025    time :  5.727\n","epoch : 2    batch : 25560    loss : 0.00021    time :  5.774\n","epoch : 2    batch : 25600    loss : 0.00256    time :  5.796\n","epoch : 2    batch : 25640    loss : 0.00036    time :  5.757\n","epoch : 2    batch : 25680    loss : 0.00346    time :  5.753\n","epoch : 2    batch : 25720    loss : 0.00055    time :  5.754\n","epoch : 2    batch : 25760    loss : 0.00042    time :  5.750\n","epoch : 2    batch : 25800    loss : 0.00581    time :  5.778\n","epoch : 2    batch : 25840    loss : 0.00114    time :  5.765\n","epoch : 2    batch : 25880    loss : 0.00489    time :  5.766\n","epoch : 2    batch : 25920    loss : 0.00208    time :  5.748\n","epoch : 2    batch : 25960    loss : 0.00078    time :  5.755\n","epoch : 2    batch : 26000    loss : 0.00064    time :  5.740\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch : 26040    loss : 0.00160    time : 11.187\n","epoch : 2    batch : 26080    loss : 0.00099    time :  5.754\n","epoch : 2    batch : 26120    loss : 0.00042    time :  5.805\n","epoch : 2    batch : 26160    loss : 0.00038    time :  5.810\n","epoch : 2    batch : 26200    loss : 0.00038    time :  5.812\n","epoch : 2    batch : 26240    loss : 0.00077    time :  5.868\n","epoch : 2    batch : 26280    loss : 0.00008    time :  5.793\n","epoch : 2    batch : 26320    loss : 0.00180    time :  5.831\n","epoch : 2    batch : 26360    loss : 0.00105    time :  5.802\n","epoch : 2    batch : 26400    loss : 0.00169    time :  5.774\n","epoch : 2    batch : 26440    loss : 0.00074    time :  5.749\n","epoch : 2    batch : 26480    loss : 0.00037    time :  5.745\n","epoch : 2    batch : 26520    loss : 0.00044    time :  5.723\n","epoch : 2    batch : 26560    loss : 0.00009    time :  5.753\n","epoch : 2    batch : 26600    loss : 0.00120    time :  5.744\n","epoch : 2    batch : 26640    loss : 0.00223    time :  5.760\n","epoch : 2    batch : 26680    loss : 0.00105    time :  5.745\n","epoch : 2    batch : 26720    loss : 0.00017    time :  5.768\n","epoch : 2    batch : 26760    loss : 0.00070    time :  5.754\n","epoch : 2    batch : 26800    loss : 0.00347    time :  5.776\n","epoch : 2    batch : 26840    loss : 0.00161    time :  5.755\n","epoch : 2    batch : 26880    loss : 0.00030    time :  5.756\n","epoch : 2    batch : 26920    loss : 0.00092    time :  5.743\n","epoch : 2    batch : 26960    loss : 0.00035    time :  5.760\n","epoch : 2    batch : 27000    loss : 0.00025    time :  5.746\n","epoch : 2    batch : 27040    loss : 0.00117    time :  5.744\n","epoch : 2    batch : 27080    loss : 0.00048    time :  5.731\n","epoch : 2    batch : 27120    loss : 0.00085    time :  6.022\n","epoch : 2    batch : 27160    loss : 0.00184    time :  5.747\n","epoch : 2    batch : 27200    loss : 0.00050    time :  5.760\n","epoch : 2    batch : 27240    loss : 0.00008    time :  5.731\n","epoch : 2    batch : 27280    loss : 0.00021    time :  5.757\n","epoch : 2    batch : 27320    loss : 0.00014    time :  5.740\n","epoch : 2    batch : 27360    loss : 0.00008    time :  5.774\n","epoch : 2    batch : 27400    loss : 0.00033    time :  5.748\n","epoch : 2    batch : 27440    loss : 0.00083    time :  5.753\n","epoch : 2    batch : 27480    loss : 0.01071    time :  5.741\n","epoch : 2    batch : 27520    loss : 0.00073    time :  5.730\n","epoch : 2    batch : 27560    loss : 0.00352    time :  5.741\n","epoch : 2    batch : 27600    loss : 0.00689    time :  5.741\n","epoch : 2    batch : 27640    loss : 0.00110    time :  5.722\n","epoch : 2    batch : 27680    loss : 0.00076    time :  5.732\n","epoch : 2    batch : 27720    loss : 0.00032    time :  5.732\n","epoch : 2    batch : 27760    loss : 0.00053    time :  5.782\n","epoch : 2    batch : 27800    loss : 0.00076    time :  5.767\n","epoch : 2    batch : 27840    loss : 0.00131    time :  5.735\n","epoch : 2    batch : 27880    loss : 0.00030    time :  5.740\n","epoch : 2    batch : 27920    loss : 0.00036    time :  5.722\n","epoch : 2    batch : 27960    loss : 0.00008    time :  5.730\n","epoch : 2    batch : 28000    loss : 0.00051    time :  5.717\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-32000.pt Data Loading Time : 155.762\n","epoch : 2    batch : 28040    loss : 0.00018    time : 167.241\n","epoch : 2    batch : 28080    loss : 0.00029    time :  5.766\n","epoch : 2    batch : 28120    loss : 0.00032    time :  5.745\n","epoch : 2    batch : 28160    loss : 0.00005    time :  5.764\n","epoch : 2    batch : 28200    loss : 0.00078    time :  5.734\n","epoch : 2    batch : 28240    loss : 0.00013    time :  5.756\n","epoch : 2    batch : 28280    loss : 0.00017    time :  5.737\n","epoch : 2    batch : 28320    loss : 0.00030    time :  5.756\n","epoch : 2    batch : 28360    loss : 0.00009    time :  5.739\n","epoch : 2    batch : 28400    loss : 0.00022    time :  5.761\n","epoch : 2    batch : 28440    loss : 0.00029    time :  5.729\n","epoch : 2    batch : 28480    loss : 0.00075    time :  5.754\n","epoch : 2    batch : 28520    loss : 0.00077    time :  5.737\n","epoch : 2    batch : 28560    loss : 0.00806    time :  5.755\n","epoch : 2    batch : 28600    loss : 0.00102    time :  5.738\n","epoch : 2    batch : 28640    loss : 0.00416    time :  5.728\n","epoch : 2    batch : 28680    loss : 0.00120    time :  5.725\n","epoch : 2    batch : 28720    loss : 0.00064    time :  5.737\n","epoch : 2    batch : 28760    loss : 0.00011    time :  5.720\n","epoch : 2    batch : 28800    loss : 0.00028    time :  5.744\n","epoch : 2    batch : 28840    loss : 0.00003    time :  5.724\n","epoch : 2    batch : 28880    loss : 0.00093    time :  5.733\n","epoch : 2    batch : 28920    loss : 0.00027    time :  5.739\n","epoch : 2    batch : 28960    loss : 0.00064    time :  5.749\n","epoch : 2    batch : 29000    loss : 0.00017    time :  5.740\n","epoch : 2    batch : 29040    loss : 0.00045    time :  5.738\n","epoch : 2    batch : 29080    loss : 0.00015    time :  5.760\n","epoch : 2    batch : 29120    loss : 0.00021    time :  5.774\n","epoch : 2    batch : 29160    loss : 0.00022    time :  5.748\n","epoch : 2    batch : 29200    loss : 0.00074    time :  5.729\n","epoch : 2    batch : 29240    loss : 0.00016    time :  5.765\n","epoch : 2    batch : 29280    loss : 0.00118    time :  5.736\n","epoch : 2    batch : 29320    loss : 0.00158    time :  5.738\n","epoch : 2    batch : 29360    loss : 0.00046    time :  5.716\n","epoch : 2    batch : 29400    loss : 0.00020    time :  5.760\n","epoch : 2    batch : 29440    loss : 0.00221    time :  5.779\n","epoch : 2    batch : 29480    loss : 0.00055    time :  5.768\n","epoch : 2    batch : 29520    loss : 0.00137    time :  5.736\n","epoch : 2    batch : 29560    loss : 0.00170    time :  5.750\n","epoch : 2    batch : 29600    loss : 0.00034    time :  5.729\n","epoch : 2    batch : 29640    loss : 0.00271    time :  5.747\n","epoch : 2    batch : 29680    loss : 0.00039    time :  5.723\n","epoch : 2    batch : 29720    loss : 0.00045    time :  5.777\n","epoch : 2    batch : 29760    loss : 0.00099    time :  6.033\n","epoch : 2    batch : 29800    loss : 0.00082    time :  5.736\n","epoch : 2    batch : 29840    loss : 0.00450    time :  5.733\n","epoch : 2    batch : 29880    loss : 0.00329    time :  5.762\n","epoch : 2    batch : 29920    loss : 0.00293    time :  5.720\n","epoch : 2    batch : 29960    loss : 0.00076    time :  5.762\n","epoch : 2    batch : 30000    loss : 0.00023    time :  5.736\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch : 30040    loss : 0.00062    time : 11.124\n","epoch : 2    batch : 30080    loss : 0.00027    time :  5.798\n","epoch : 2    batch : 30120    loss : 0.00239    time :  5.838\n","epoch : 2    batch : 30160    loss : 0.00027    time :  5.915\n","epoch : 2    batch : 30200    loss : 0.00400    time :  6.001\n","epoch : 2    batch : 30240    loss : 0.00345    time :  5.909\n","epoch : 2    batch : 30280    loss : 0.00092    time :  5.918\n","epoch : 2    batch : 30320    loss : 0.00114    time :  5.941\n","epoch : 2    batch : 30360    loss : 0.00405    time :  5.954\n","epoch : 2    batch : 30400    loss : 0.00073    time :  5.897\n","epoch : 2    batch : 30440    loss : 0.00034    time :  5.823\n","epoch : 2    batch : 30480    loss : 0.00003    time :  5.798\n","epoch : 2    batch : 30520    loss : 0.00004    time :  5.756\n","epoch : 2    batch : 30560    loss : 0.00084    time :  5.743\n","epoch : 2    batch : 30600    loss : 0.00077    time :  5.716\n","epoch : 2    batch : 30640    loss : 0.00011    time :  5.730\n","epoch : 2    batch : 30680    loss : 0.00003    time :  5.736\n","epoch : 2    batch : 30720    loss : 0.00097    time :  5.732\n","epoch : 2    batch : 30760    loss : 0.00013    time :  5.745\n","epoch : 2    batch : 30800    loss : 0.00077    time :  5.743\n","epoch : 2    batch : 30840    loss : 0.00025    time :  5.738\n","epoch : 2    batch : 30880    loss : 0.00015    time :  5.748\n","epoch : 2    batch : 30920    loss : 0.00060    time :  5.730\n","epoch : 2    batch : 30960    loss : 0.00101    time :  5.742\n","epoch : 2    batch : 31000    loss : 0.00034    time :  5.739\n","epoch : 2    batch : 31040    loss : 0.00134    time :  5.759\n","epoch : 2    batch : 31080    loss : 0.00225    time :  5.743\n","epoch : 2    batch : 31120    loss : 0.00051    time :  5.731\n","epoch : 2    batch : 31160    loss : 0.00062    time :  5.738\n","epoch : 2    batch : 31200    loss : 0.00027    time :  5.734\n","epoch : 2    batch : 31240    loss : 0.00035    time :  5.733\n","epoch : 2    batch : 31280    loss : 0.00011    time :  5.724\n","epoch : 2    batch : 31320    loss : 0.00034    time :  5.727\n","epoch : 2    batch : 31360    loss : 0.00094    time :  5.717\n","epoch : 2    batch : 31400    loss : 0.00032    time :  5.749\n","epoch : 2    batch : 31440    loss : 0.00017    time :  5.722\n","epoch : 2    batch : 31480    loss : 0.00027    time :  5.740\n","epoch : 2    batch : 31520    loss : 0.00081    time :  5.739\n","epoch : 2    batch : 31560    loss : 0.00020    time :  5.737\n","epoch : 2    batch : 31600    loss : 0.00103    time :  5.730\n","epoch : 2    batch : 31640    loss : 0.00072    time :  5.726\n","epoch : 2    batch : 31680    loss : 0.00036    time :  5.727\n","epoch : 2    batch : 31720    loss : 0.00015    time :  5.751\n","epoch : 2    batch : 31760    loss : 0.00119    time :  5.730\n","epoch : 2    batch : 31800    loss : 0.00411    time :  5.734\n","epoch : 2    batch : 31840    loss : 0.00044    time :  5.746\n","epoch : 2    batch : 31880    loss : 0.00011    time :  5.774\n","epoch : 2    batch : 31920    loss : 0.00243    time :  5.726\n","epoch : 2    batch : 31960    loss : 0.00041    time :  5.744\n","epoch : 2    batch : 32000    loss : 0.00102    time :  5.727\n","\n","Saving Model ...\n","Saving Finished\n","\n","Data Loading\n","data_torch-36000.pt Data Loading Time : 105.418\n","epoch : 2    batch : 32040    loss : 0.00004    time : 116.515\n","epoch : 2    batch : 32080    loss : 0.00006    time :  5.730\n","epoch : 2    batch : 32120    loss : 0.00086    time :  5.763\n","epoch : 2    batch : 32160    loss : 0.00240    time :  5.742\n","epoch : 2    batch : 32200    loss : 0.00016    time :  5.740\n","epoch : 2    batch : 32240    loss : 0.00017    time :  5.724\n","epoch : 2    batch : 32280    loss : 0.00034    time :  5.733\n","epoch : 2    batch : 32320    loss : 0.00059    time :  5.726\n","epoch : 2    batch : 32360    loss : 0.00866    time :  5.753\n","epoch : 2    batch : 32400    loss : 0.00328    time :  5.737\n","epoch : 2    batch : 32440    loss : 0.00031    time :  5.744\n","epoch : 2    batch : 32480    loss : 0.00088    time :  5.731\n","epoch : 2    batch : 32520    loss : 0.00125    time :  5.726\n","epoch : 2    batch : 32560    loss : 0.00114    time :  5.736\n","epoch : 2    batch : 32600    loss : 0.00038    time :  5.772\n","epoch : 2    batch : 32640    loss : 0.00043    time :  5.757\n","epoch : 2    batch : 32680    loss : 0.00034    time :  5.736\n","epoch : 2    batch : 32720    loss : 0.00027    time :  5.719\n","epoch : 2    batch : 32760    loss : 0.00070    time :  5.755\n","epoch : 2    batch : 32800    loss : 0.00042    time :  5.740\n","epoch : 2    batch : 32840    loss : 0.00139    time :  5.768\n","epoch : 2    batch : 32880    loss : 0.00096    time :  5.756\n","epoch : 2    batch : 32920    loss : 0.00144    time :  5.764\n","epoch : 2    batch : 32960    loss : 0.00024    time :  5.723\n","epoch : 2    batch : 33000    loss : 0.00470    time :  5.740\n","epoch : 2    batch : 33040    loss : 0.00135    time :  5.731\n","epoch : 2    batch : 33080    loss : 0.00208    time :  5.761\n","epoch : 2    batch : 33120    loss : 0.00095    time :  5.738\n","epoch : 2    batch : 33160    loss : 0.00053    time :  5.748\n","epoch : 2    batch : 33200    loss : 0.00248    time :  5.734\n","epoch : 2    batch : 33240    loss : 0.00028    time :  5.977\n","epoch : 2    batch : 33280    loss : 0.00099    time :  5.742\n","epoch : 2    batch : 33320    loss : 0.00056    time :  5.734\n","epoch : 2    batch : 33360    loss : 0.00055    time :  5.740\n","epoch : 2    batch : 33400    loss : 0.00198    time :  5.771\n","epoch : 2    batch : 33440    loss : 0.00647    time :  5.735\n","epoch : 2    batch : 33480    loss : 0.01697    time :  5.749\n","epoch : 2    batch : 33520    loss : 0.00090    time :  5.768\n","epoch : 2    batch : 33560    loss : 0.00353    time :  5.735\n","epoch : 2    batch : 33600    loss : 0.00252    time :  5.766\n","epoch : 2    batch : 33640    loss : 0.00019    time :  5.728\n","epoch : 2    batch : 33680    loss : 0.00042    time :  5.732\n","epoch : 2    batch : 33720    loss : 0.00026    time :  5.722\n","epoch : 2    batch : 33760    loss : 0.00070    time :  5.749\n","epoch : 2    batch : 33800    loss : 0.00028    time :  5.720\n","epoch : 2    batch : 33840    loss : 0.00179    time :  5.748\n","epoch : 2    batch : 33880    loss : 0.00014    time :  5.739\n","epoch : 2    batch : 33920    loss : 0.00044    time :  5.749\n","epoch : 2    batch : 33960    loss : 0.00026    time :  5.734\n","epoch : 2    batch : 34000    loss : 0.00019    time :  5.751\n","\n","Saving Model ...\n","Saving Finished\n","\n","epoch : 2    batch : 34040    loss : 0.00036    time : 11.463\n","epoch : 2    batch : 34080    loss : 0.00102    time :  5.749\n","epoch : 2    batch : 34120    loss : 0.00176    time :  5.792\n","epoch : 2    batch : 34160    loss : 0.00160    time :  5.798\n","epoch : 2    batch : 34200    loss : 0.00058    time :  5.831\n","epoch : 2    batch : 34240    loss : 0.00037    time :  5.838\n","epoch : 2    batch : 34280    loss : 0.00106    time :  5.778\n","epoch : 2    batch : 34320    loss : 0.00208    time :  5.733\n","epoch : 2    batch : 34360    loss : 0.00089    time :  5.786\n","epoch : 2    batch : 34400    loss : 0.00022    time :  5.729\n","epoch : 2    batch : 34440    loss : 0.00229    time :  5.740\n","epoch : 2    batch : 34480    loss : 0.00227    time :  5.732\n","epoch : 2    batch : 34520    loss : 0.00132    time :  5.737\n","epoch : 2    batch : 34560    loss : 0.00146    time :  5.730\n","epoch : 2    batch : 34600    loss : 0.00199    time :  5.748\n","epoch : 2    batch : 34640    loss : 0.01773    time :  5.739\n"]}],"source":["## Main Code\n","test_name = 'feature_dim128'\n","\n","# initial path\n","student_model_save_path = f'/content/drive/MyDrive/Colab Notebooks/Model/ShowRoom/.model/{test_name}'\n","rooms_path = '/content/drive/MyDrive/Scannet++/data_scannet_r_3'\n","\n","# -- 1.  Load Data Path\n","rooms_name = [it for it in os.listdir('/content/drive/MyDrive/Scannet++') if it.endswith('.pt')][:9]\n","print(f'Number of Train Data : {len(rooms_name)*1000}')\n","\n","# -- 2. Load Teahcer Model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'device : {device} \\n')\n"," # Fast3r 제공 모델\n","torch.manual_seed(42) ; torch.cuda.manual_seed(42)\n","print('Building Teacher model ...')\n","teacher_model = Fast3R(**teacher_args)\n","teacher_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Model/ShowRoom/.model/teacher.pth'))\n","teacher_model = teacher_model.to(device)\n","print('Finished Building Teacher model ! \\n')\n","\n","# -- 3. Build Students model\n","print('Building Student model ...')\n","student_model = Fast3R(**student_args)\n","student_model = student_model.to(device)\n","print('Finished Building Student model ! \\n')\n","\n"," # Sho3r 모델 저장 경로\n","os.makedirs(student_model_save_path, exist_ok=True)\n","\n"," # load pre_model\n","if os.listdir(student_model_save_path):\n","    last_student_model = sorted([it for it in os.listdir(student_model_save_path)])[-1]\n","    student_model.load_state_dict(torch.load(student_model_save_path + f'/{last_student_model}'))\n","    print(f'loaded : {last_student_model} \\n')\n","    start_epoch = int(re.findall(r'\\d+', last_student_model)[0])\n","    start_i, start_batch = int(re.findall(r'\\d+', last_student_model)[1]) // 4000, (int(re.findall(r'\\d+', last_student_model)[1]) % 4000) // 4 + 1\n","    if start_i == len(rooms_name):\n","        start_i, start_batch = 0, 1\n","        start_epoch += 1\n","else:\n","    student_model.encoder = teacher_model.encoder\n","    student_model.decoder = teacher_model.decoder\n","    print(\"loaded : Teacher's en/decoder \\n\")\n","    start_epoch, start_i, start_batch = 1, 0, 1\n","\n","# -- 4. Set Prams\n","epoch = 3\n","\n","learning_rate = 1e-5\n","batch_size = 4\n","kd_loss = RKDLoss1()\n","optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate\n","                            , weight_decay=0.05, betas=(0.9, 0.95))\n","\n","# -- 5. Train\n","for e in range(start_epoch, epoch+1):\n","    print(f'########### START epoch : {e} ###########')\n","\n","    teacher_model.eval()\n","    student_model.train()\n","    start_time = time.time()\n","    for i in range(start_i, (len(rooms_name))):\n","\n","        print('Data Loading')\n","        load_time = time.time()\n","        data_all = torch.load(f'/content/drive/MyDrive/Scannet++/{rooms_name[i]}')\n","        print(f'{rooms_name[i]} Data Loading Time : {time.time() - load_time:6.3f}')\n","\n","        for it in range(start_batch, 1001):\n","            # 데이터 가져오기\n","            batch_data = data_all.pop()\n","\n","            optimizer.zero_grad()\n","            # prediction\n","            with torch.no_grad():\n","                teacher_pred = teacher_model(batch_data)\n","            student_pred = student_model(batch_data)\n","\n","            # loss\n","\n","            train_loss = kd_loss(student_pred, teacher_pred)\n","            train_loss.backward()\n","            optimizer.step()\n","\n","            if it % 10 == 0:\n","                current_time =  time.time() - start_time\n","                info = f'epoch : {e}    batch : {it * batch_size + i * 4000:5d}    loss : {train_loss.item():.5f}    time : {current_time:6.3f}'\n","                print(info)\n","                with open(f'{student_model_save_path}/information_{test_name}.txt', 'a') as f:\n","                    f.write(info + '\\n')\n","                start_time = time.time()\n","\n","            if it % 500 == 0:\n","                print('\\nSaving Model ...')\n","                torch.save(student_model.state_dict(), f'{student_model_save_path}/model_{e}_{(it * batch_size + i * 4000):05d}.pth')\n","                print('Saving Finished\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pocSlhsRNNa"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RKDLoss1(nn.Module):\n","    def __init__(self, distance_weight=1, angle_weight=2):\n","        super(RKDLoss1, self).__init__()\n","        self.distance_weight = distance_weight\n","        self.angle_weight = angle_weight\n","\n","    def pdist(self, e, eps=1e-8):\n","        # e = [b, c']\n","        e_square = e.pow(2).sum(dim=1) # b\n","        prod = e @ e.t() # [b, b]\n","        res = (e_square.unsqueeze(1) + e_square.unsqueeze(0) - 2 * prod).clamp(min=eps) # [b, 1] + [1, b] - 2 * [b, b]\n","        # [b, b]\n","\n","        res = res.sqrt() # [b, b]\n","\n","        res = res.clone()\n","        res[range(len(e)), range(len(e))] = 0 # 자신은 0\n","        return res\n","\n","    def RKDDistance(self, student, teacher):\n","        # Input [b, c']\n","        with torch.no_grad():\n","            t_d = self.pdist(teacher)\n","            mean_td = t_d[t_d > 0].mean()\n","            t_d = t_d / mean_td\n","\n","        d = self.pdist(student)\n","        mean_d = d[d > 0].mean()\n","        d = d / mean_d\n","\n","        loss = F.smooth_l1_loss(d, t_d, reduction='mean')\n","        return loss\n","\n","\n","    def RKDAngle(self, student, teacher):\n","        \"\"\"\n","        student, teacher: [N, D] 텐서, N: 배치 크기, D: 특징 차원\n","        각도 관계 손실 계산\n","        \"\"\"\n","        with torch.no_grad():\n","            td = (teacher.unsqueeze(0) - teacher.unsqueeze(1)) # [1, b, c'] - [b, 1, c'], [b, b, c']\n","            norm_td = F.normalize(td, p=2, dim=2) # [b, b, c']\n","            t_angle = torch.bmm(norm_td, norm_td.transpose(1, 2)).view(-1) # [b*c'*b]\n","\n","        sd = (student.unsqueeze(0) - student.unsqueeze(1))\n","        norm_sd = F.normalize(sd, p=2, dim=2)\n","        s_angle = torch.bmm(norm_sd, norm_sd.transpose(1, 2)).view(-1)\n","\n","        loss = F.smooth_l1_loss(s_angle, t_angle, reduction='mean')\n","        return loss\n","\n","    def process_model_output(self, model_output):\n","        features = []\n","        # 모델 출력의 각 항목 처리\n","        for output_dict in model_output:\n","            # Global pointmap과 confidence 처리\n","            pointmap = output_dict['pts3d_in_other_view']  # [b, w, h, c]\n","            conf = output_dict['conf']  # [b, w, h]\n","\n","            if len(pointmap.shape) == 4 and len(conf.shape) == 3:\n","                pointmap = pointmap.permute(0, 3, 2, 1)  # [b, c, h, w]\n","                conf = conf.permute(0, 2, 1).unsqueeze(1)  # [b, 1, h, w]\n","\n","                # Confidence로 가중치 부여\n","                weighted_features = pointmap * conf  # [b, c, h, w]\n","\n","                # Confidence의 합으로 나누어 정규화 (0으로 나누는 것 방지)\n","                conf_sum = conf.sum(dim=(2, 3), keepdim=True).clamp(min=1e-6)\n","                normalized_features = (weighted_features.sum(dim=(2, 3)) / conf_sum.squeeze(3).squeeze(2))  # [b, c]\n","\n","                features.append(normalized_features)\n","\n","        # 모든 특징을 연결\n","        if features:\n","            combined_features = torch.cat(features, dim=1)  # [b, s*c]\n","            return combined_features\n","        else:\n","            raise ValueError(\"특징 추출에 실패했습니다. 모델 출력 형식을 확인하세요.\")\n","\n","    def forward(self, student_output, teacher_output):\n","        student_features = self.process_model_output(student_output)  # [b, s*c]\n","        teacher_features = self.process_model_output(teacher_output)  # [b, s*c]\n","\n","        # 거리 손실\n","        dist_loss = self.RKDDistance(student_features, teacher_features)\n","\n","        # 각도 손실\n","        angle_loss = self.RKDAngle(student_features, teacher_features)\n","\n","        # 가중치 적용한 최종 손실\n","        loss = self.distance_weight * dist_loss + self.angle_weight * angle_loss\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmR9o5ZvyQUK"},"outputs":[],"source":["################ 시각화 ######################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhOf2A6zMTdH"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","from fast3r.dust3r.utils.image import load_images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":8386,"status":"ok","timestamp":1747383979472,"user":{"displayName":"지능정보 SW아카데미5조","userId":"16997830524223997531"},"user_tz":-540},"id":"kCC2Be_iMWm5","outputId":"6aa3e830-351f-403b-edaf-0535082784b9"},"outputs":[{"name":"stdout","output_type":"stream","text":[">> Loading images from /content/a\n"," - adding 0.jpg with resolution 720x540 --> 512x384\n"," - adding 1.jpg with resolution 720x540 --> 512x384\n"," - adding 2.jpg with resolution 720x540 --> 512x384\n"," (Found 3 images)\n",">> Inference with model on 3 images\n","encode_images time: 0.06508588790893555\n","pos emb time: 0.0007193088531494141\n","decoder time: 0.02459883689880371\n","head prepare input time: 0.0003228187561035156\n","head forward time: 0.014707803726196289\n","total Fast3R forward time: 0.10555195808410645\n","Camera Pose for view 0:\n","(4, 4)\n","Camera Pose for view 1:\n","(4, 4)\n","Camera Pose for view 2:\n","(4, 4)\n","Point Cloud Shape for view 0: (1, 384, 512, 3)\n","Point Cloud Shape for view 1: (1, 384, 512, 3)\n","Point Cloud Shape for view 2: (1, 384, 512, 3)\n"]}],"source":["student_model = Fast3R(**student_args)\n","student_model = student_model.to(device)\n","student_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Model/ShowRoom/.model/Half_Head_1/model_2_20000.pth'))\n","\n","# teacher_model = Fast3R(**teacher_args)\n","# teacher_model = teacher_model.to(device)\n","# teacher_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Model/ShowRoom/.model/teacher.pth'))\n","\n","model = student_model\n","image_path = '/content/a'\n","images = load_images(image_path, size=512, verbose=True)\n","\n","model.eval()\n","lit_module = MultiViewDUSt3RLitModule.load_for_inference(model)\n","lit_module.eval()\n","\n","output_dict, profiling_info = inference(\n","    images,\n","    model,\n","    device,\n","    dtype=torch.float32,\n","    verbose=True,\n","    profiling=True,\n",")\n","\n","poses_c2w_batch, estimated_focals = MultiViewDUSt3RLitModule.estimate_camera_poses(\n","    output_dict['preds'],\n","    niter_PnP=100,\n","    focal_length_estimation_method='first_view_from_global_head'\n",")\n","\n","\n","camera_poses = poses_c2w_batch[0]\n","\n","for view_idx, pose in enumerate(camera_poses):\n","    print(f\"Camera Pose for view {view_idx}:\")\n","    print(pose.shape)  # np.array of shape (4, 4), the camera-to-world transformation matrix\n","\n","for view_idx, pred in enumerate(output_dict['preds']):\n","    point_cloud = pred['pts3d_in_other_view'].cpu().numpy()\n","    print(f\"Point Cloud Shape for view {view_idx}: {point_cloud.shape}\")  # shape: (1, 368, 512, 3), i.e., (1, Height, Width, XYZ)  # shape: (b, 368, 512, 3), i.e., (1, Height, Width, XYZ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":293,"referenced_widgets":["dcf1e338d7274d1da3fc228ebae957ad","d06906538cc84877ba32b433cdca4d3c"]},"executionInfo":{"elapsed":2781,"status":"ok","timestamp":1747383987186,"user":{"displayName":"지능정보 SW아카데미5조","userId":"16997830524223997531"},"user_tz":-540},"id":"3M5QUA1KS_8N","outputId":"b22f469e-2140-49d5-d782-18488faf22ab"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────── <span style=\"font-weight: bold\">viser</span> ────────────────╮\n","│             ╷                         │\n","│   HTTP      │ http://127.0.0.1:8023   │\n","│   Websocket │ ws://127.0.0.1:8023     │\n","│             ╵                         │\n","╰───────────────────────────────────────╯\n","</pre>\n"],"text/plain":["╭──────────────── \u001b[1mviser\u001b[0m ────────────────╮\n","│             ╷                         │\n","│   HTTP      │ http://127.0.0.1:8023   │\n","│   Websocket │ ws://127.0.0.1:8023     │\n","│             ╵                         │\n","╰───────────────────────────────────────╯\n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcf1e338d7274d1da3fc228ebae957ad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d06906538cc84877ba32b433cdca4d3c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Scene type detection:\n","- Found 0/5 frames with significant sky presence (>20% sky pixels)\n","- Scene classified as: indoor, setting mask_sky to False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(viser)</span> Share URL requested!\n","</pre>\n"],"text/plain":["\u001b[1m(\u001b[0m\u001b[1mviser\u001b[0m\u001b[1m)\u001b[0m Share URL requested!\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(viser)</span> Generated share URL <span style=\"font-weight: bold\">(</span>expires in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span> hours, max <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> clients<span style=\"font-weight: bold\">)</span>: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arm-tracker.share.viser.studio</span>\n","</pre>\n"],"text/plain":["\u001b[1m(\u001b[0m\u001b[1mviser\u001b[0m\u001b[1m)\u001b[0m Generated share URL \u001b[1m(\u001b[0mexpires in \u001b[1;36m24\u001b[0m hours, max \u001b[1;36m16\u001b[0m clients\u001b[1m)\u001b[0m: \u001b[4;94mhttps://arm-tracker.share.viser.studio\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["🌐 3D 시각화 링크: https://arm-tracker.share.viser.studio\n"]}],"source":["# --- Align local point clouds to global space ---\n","lit_module.align_local_pts3d_to_global(\n","    preds=output_dict['preds'],\n","    views=output_dict['views'],\n","    min_conf_thr_percentile=85\n",")\n","\n","from fast3r.viz.viser_visualizer import start_visualization\n","\n","server = start_visualization(\n","    output=output_dict,\n","    min_conf_thr_percentile=10,\n","    global_conf_thr_value_to_drop_view=1.5,\n","    point_size=0.0004,\n",")\n","\n","print(\"🌐 3D 시각화 링크:\", server.request_share_url())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjj1zU-Blxsj"},"outputs":[],"source":["teacher_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3X4ed3Bib8GY"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"19KG_CEo_r0D2ENoJqNSLcoll6Sf6P705","authorship_tag":"ABX9TyP+Ir5Kqubq2HA6CQ5YRYhE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}